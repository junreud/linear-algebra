"""
BERT Encoder + Transformer Decoder í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (HuggingFace ë°ì´í„°ì…‹ ë²„ì „)

ì‹¤ì œ ë²ˆì—­ ë°ì´í„°ì…‹(WMT, OPUS ë“±)ì„ ì‚¬ìš©í•˜ì—¬ Encoder-Decoder ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python train_encoder_decoder_hf.py --dataset opus_books --src_lang en --tgt_lang ko --epochs 5
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch.nn.functional as F

from transformers import AutoTokenizer
from datasets import load_dataset
from encoder_decoder_model import BERTEncoderTransformerDecoderModel, EncoderDecoderConfig
import os
import json
from tqdm import tqdm
import numpy as np
import argparse
from typing import Dict, List, Optional, Union


class HuggingFaceTranslationDataset(Dataset):
    """
    HuggingFace ë²ˆì—­ ë°ì´í„°ì…‹
    
    ì§€ì› ë°ì´í„°ì…‹:
    - opus_books: ë‹¤êµ­ì–´ ë„ì„œ ë²ˆì—­ (ê³ í’ˆì§ˆ)
    - wmt16: WMT 2016 ë²ˆì—­ ëŒ€íšŒ ë°ì´í„°
    - wmt14: WMT 2014 ë²ˆì—­ ëŒ€íšŒ ë°ì´í„°  
    - opus100: 100ê°œ ì–¸ì–´ ë²ˆì—­ ë°ì´í„°
    - kde4: KDE ì†Œí”„íŠ¸ì›¨ì–´ ë²ˆì—­
    """
    
    def __init__(
        self,
        dataset_name: str = "opus_books",
        src_lang: str = "en",
        tgt_lang: str = "ko", 
        tokenizer_name: str = "bert-base-multilingual-cased",
        max_length: int = 128,
        num_samples: Optional[int] = None,
        split: str = "train"
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        
        print(f"ğŸ“š HuggingFace ë²ˆì—­ ë°ì´í„°ì…‹ ë¡œë”©: {dataset_name} ({src_lang} â†’ {tgt_lang})")
        
        # ë°ì´í„°ì…‹ë³„ ë¡œë”© ë°©ì‹
        if dataset_name == "opus_books":
            dataset = load_dataset("opus_books", f"{src_lang}-{tgt_lang}", split=split)
            
        elif dataset_name.startswith("wmt"):
            # WMT ë°ì´í„°ì…‹
            if dataset_name == "wmt16":
                dataset = load_dataset("wmt16", f"{src_lang}-{tgt_lang}", split=split)
            elif dataset_name == "wmt14":
                dataset = load_dataset("wmt14", f"{src_lang}-{tgt_lang}", split=split)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” WMT ë°ì´í„°ì…‹: {dataset_name}")
                
        elif dataset_name == "opus100":
            dataset = load_dataset("opus100", f"{src_lang}-{tgt_lang}", split=split)
            
        elif dataset_name == "kde4":
            dataset = load_dataset("kde4", f"{src_lang}-{tgt_lang}", split=split)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_name}")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if num_samples and len(dataset) > num_samples:
            dataset = dataset.select(range(num_samples))
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ë²ˆì—­ ìŒ")
        
        # ë²ˆì—­ ìŒ ì¶”ì¶œ
        self.translation_pairs = []
        
        for item in tqdm(dataset, desc="ë²ˆì—­ ìŒ ì¶”ì¶œ"):
            if dataset_name == "opus_books":
                src_text = item['translation'][src_lang]
                tgt_text = item['translation'][tgt_lang]
            elif dataset_name.startswith("wmt"):
                src_text = item['translation'][src_lang]
                tgt_text = item['translation'][tgt_lang]
            elif dataset_name == "opus100":
                src_text = item['translation'][src_lang]
                tgt_text = item['translation'][tgt_lang]
            elif dataset_name == "kde4":
                src_text = item['translation'][src_lang]
                tgt_text = item['translation'][tgt_lang]
            
            # ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ìŠ¤í‚µ
            if not src_text.strip() or not tgt_text.strip():
                continue
            if len(src_text.split()) < 3 or len(tgt_text.split()) < 3:
                continue
            if len(src_text) > 500 or len(tgt_text) > 500:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ìŠ¤í‚µ
                continue
                
            self.translation_pairs.append((src_text.strip(), tgt_text.strip()))
        
        print(f"âœ… ìœ íš¨í•œ ë²ˆì—­ ìŒ: {len(self.translation_pairs)}ê°œ")
    
    def __len__(self):
        return len(self.translation_pairs)
    
    def __getitem__(self, idx):
        src_text, tgt_text = self.translation_pairs[idx]
        
        # Encoder ì…ë ¥ (ì†ŒìŠ¤ ì–¸ì–´)
        encoder_inputs = self.tokenizer(
            src_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Decoder ì…ë ¥ ë° ë ˆì´ë¸” (íƒ€ê²Ÿ ì–¸ì–´)
        # Teacher Forcing: ì…ë ¥ì€ [BOS] + í† í°ë“¤, ë ˆì´ë¸”ì€ í† í°ë“¤ + [EOS]
        tgt_with_bos = f"[CLS] {tgt_text}"  # BOS í† í° ì¶”ê°€
        tgt_with_eos = f"{tgt_text} [SEP]"  # EOS í† í° ì¶”ê°€
        
        decoder_inputs = self.tokenizer(
            tgt_with_bos,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        labels = self.tokenizer(
            tgt_with_eos,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoder_inputs['input_ids'].squeeze(0),
            'attention_mask': encoder_inputs['attention_mask'].squeeze(0),
            'decoder_input_ids': decoder_inputs['input_ids'].squeeze(0),
            'decoder_attention_mask': decoder_inputs['attention_mask'].squeeze(0),
            'labels': labels['input_ids'].squeeze(0),
            'src_text': src_text,
            'tgt_text': tgt_text,
        }


class EncoderDecoderTrainerHF:
    """HuggingFace ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ Encoder-Decoder ëª¨ë¸ í›ˆë ¨ê¸°"""
    
    def __init__(
        self, 
        model, 
        tokenizer, 
        train_dataset, 
        val_dataset=None,
        batch_size=4,
        learning_rate=1e-4,
        weight_decay=0.01,
        gradient_accumulation_steps=4,
        max_grad_norm=1.0,
        device='cpu'
    ):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_grad_norm = max_grad_norm
        
        # ë°ì´í„° ë¡œë”
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0,  # macOS í˜¸í™˜ì„±
            pin_memory=True if device != 'cpu' else False
        )
        
        self.val_loader = None
        if val_dataset:
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=True if device != 'cpu' else False
            )
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = AdamW(
            model.parameters(),
            lr=learning_rate,
            betas=(0.9, 0.98),
            eps=1e-9,
            weight_decay=weight_decay
        )
        
        total_steps = len(self.train_loader) * 10  # ì¶”ì •ì¹˜
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=learning_rate * 0.1
        )
        
        self.train_losses = []
        self.val_losses = []
        self.train_bleu_scores = []
        self.val_bleu_scores = []
    
    def calculate_bleu(self, predictions: List[str], references: List[str]) -> float:
        """ê°„ë‹¨í•œ BLEU ì ìˆ˜ ê³„ì‚°"""
        try:
            from nltk.translate.bleu_score import corpus_bleu
            from nltk.tokenize import word_tokenize
            
            # í† í¬ë‚˜ì´ì§•
            tokenized_preds = [word_tokenize(pred.lower()) for pred in predictions]
            tokenized_refs = [[word_tokenize(ref.lower())] for ref in references]
            
            # BLEU ì ìˆ˜ ê³„ì‚°
            bleu_score = corpus_bleu(tokenized_refs, tokenized_preds)
            return bleu_score
        except ImportError:
            # NLTKê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë‹¨ì–´ ì¼ì¹˜ìœ¨ ê³„ì‚°
            total_matches = 0
            total_words = 0
            
            for pred, ref in zip(predictions, references):
                pred_words = set(pred.lower().split())
                ref_words = set(ref.lower().split())
                matches = len(pred_words & ref_words)
                total_matches += matches
                total_words += len(ref_words)
            
            return total_matches / max(total_words, 1)
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        predictions = []
        references = []
        
        progress_bar = tqdm(self.train_loader, desc=f"Training Epoch {epoch}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                    for k, v in batch.items()}
            
            # ìˆœì „íŒŒ
            outputs = self.model(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                decoder_input_ids=batch['decoder_input_ids'],
                decoder_attention_mask=batch['decoder_attention_mask'],
                labels=batch['labels'],
                return_dict=True
            )
            
            loss = outputs.loss
            
            # Gradient Accumulation
            loss = loss / self.gradient_accumulation_steps
            loss.backward()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item() * self.gradient_accumulation_steps
            num_batches += 1
            
            # BLEU ì ìˆ˜ ê³„ì‚°ìš© ì˜ˆì¸¡ ìˆ˜ì§‘ (ì¼ë¶€ë§Œ)
            if batch_idx % 20 == 0:  # 20ë°°ì¹˜ë§ˆë‹¤ BLEU ê³„ì‚°
                with torch.no_grad():
                    generated_ids = self.model.generate(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        max_length=50,
                        do_sample=False
                    )
                    
                    batch_predictions = self.tokenizer.batch_decode(
                        generated_ids, skip_special_tokens=True
                    )
                    batch_references = batch['tgt_text']
                    
                    predictions.extend(batch_predictions)
                    references.extend(batch_references)
            
            # Gradient ì—…ë°ì´íŠ¸
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.max_grad_norm
                )
                
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
            
            # ì§„í–‰ë¥  í‘œì‹œ
            avg_loss = total_loss / num_batches
            progress_bar.set_postfix({
                'loss': f"{avg_loss:.4f}",
                'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
            })
        
        # BLEU ì ìˆ˜ ê³„ì‚°
        bleu_score = self.calculate_bleu(predictions, references) if predictions else 0.0
        
        return {
            'loss': total_loss / num_batches,
            'bleu': bleu_score
        }
    
    def validate(self) -> Optional[Dict[str, float]]:
        """ê²€ì¦"""
        if self.val_loader is None:
            return None
        
        self.model.eval()
        total_loss = 0
        num_batches = 0
        predictions = []
        references = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    decoder_input_ids=batch['decoder_input_ids'],
                    decoder_attention_mask=batch['decoder_attention_mask'],
                    labels=batch['labels'],
                    return_dict=True
                )
                
                total_loss += outputs.loss.item()
                num_batches += 1
                
                # ë²ˆì—­ ìƒì„±
                generated_ids = self.model.generate(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    max_length=50,
                    do_sample=False
                )
                
                batch_predictions = self.tokenizer.batch_decode(
                    generated_ids, skip_special_tokens=True
                )
                batch_references = batch['tgt_text']
                
                predictions.extend(batch_predictions)
                references.extend(batch_references)
        
        val_loss = total_loss / num_batches
        bleu_score = self.calculate_bleu(predictions, references)
        
        return {
            'loss': val_loss,
            'bleu': bleu_score,
            'sample_predictions': predictions[:5],
            'sample_references': references[:5]
        }
    
    def train(self, num_epochs: int, save_dir: str = "encoder_decoder_checkpoints_hf"):
        """ì „ì²´ í›ˆë ¨ ë£¨í”„"""
        print(f"ğŸš€ Encoder-Decoder ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (HuggingFace ë°ì´í„°)")
        print(f"ğŸ“Š í›ˆë ¨ ì„¤ì •:")
        print(f"   - ì—í¬í¬: {num_epochs}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.train_loader.batch_size}")
        print(f"   - Gradient Accumulation: {self.gradient_accumulation_steps}")
        print(f"   - ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: {self.train_loader.batch_size * self.gradient_accumulation_steps}")
        print(f"   - í•™ìŠµë¥ : {self.optimizer.param_groups[0]['lr']}")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}")
        
        os.makedirs(save_dir, exist_ok=True)
        best_val_bleu = 0.0
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“ˆ Epoch {epoch + 1}/{num_epochs}")
            
            # í›ˆë ¨
            train_metrics = self.train_epoch(epoch + 1)
            self.train_losses.append(train_metrics['loss'])
            self.train_bleu_scores.append(train_metrics['bleu'])
            
            # ê²€ì¦
            val_metrics = self.validate()
            if val_metrics:
                self.val_losses.append(val_metrics['loss'])
                self.val_bleu_scores.append(val_metrics['bleu'])
                
                print(f"âœ… Train Loss: {train_metrics['loss']:.4f}, BLEU: {train_metrics['bleu']:.4f}")
                print(f"âœ… Val Loss: {val_metrics['loss']:.4f}, BLEU: {val_metrics['bleu']:.4f}")
                
                # ìƒ˜í”Œ ë²ˆì—­ ì¶œë ¥
                print(f"\nğŸ” ë²ˆì—­ ìƒ˜í”Œ:")
                for i, (pred, ref) in enumerate(zip(val_metrics['sample_predictions'][:3], 
                                                   val_metrics['sample_references'][:3])):
                    print(f"   {i+1}. ì˜ˆì¸¡: {pred}")
                    print(f"      ì •ë‹µ: {ref}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_metrics['bleu'] > best_val_bleu:
                    best_val_bleu = val_metrics['bleu']
                    self.save_checkpoint(os.path.join(save_dir, f"best_model.pt"), epoch + 1)
                    print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (BLEU: {val_metrics['bleu']:.4f})")
            else:
                print(f"âœ… Train Loss: {train_metrics['loss']:.4f}, BLEU: {train_metrics['bleu']:.4f}")
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (epoch + 1) % 2 == 0:
                self.save_checkpoint(os.path.join(save_dir, f"checkpoint_epoch_{epoch+1}.pt"), epoch + 1)
        
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_bleu_scores': self.train_bleu_scores,
            'val_bleu_scores': self.val_bleu_scores
        }
    
    def save_checkpoint(self, filepath: str, epoch: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_bleu_scores': self.train_bleu_scores,
            'val_bleu_scores': self.val_bleu_scores,
            'config': self.model.config,
        }, filepath)
    
    def translate_sample(self, source_text: str, max_length: int = 50) -> str:
        """ìƒ˜í”Œ ë²ˆì—­ ìƒì„±"""
        self.model.eval()
        
        with torch.no_grad():
            inputs = self.tokenizer(
                source_text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=128
            ).to(self.device)
            
            generated_ids = self.model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_length=max_length,
                do_sample=False
            )
            
            translation = self.tokenizer.decode(
                generated_ids[0], 
                skip_special_tokens=True
            )
            
            return translation


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Encoder-Decoder ë²ˆì—­ ëª¨ë¸ í›ˆë ¨ (HuggingFace ë°ì´í„°)")
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument('--dataset', type=str, default='opus_books',
                       choices=['opus_books', 'wmt16', 'wmt14', 'opus100', 'kde4'],
                       help='ì‚¬ìš©í•  HuggingFace ë²ˆì—­ ë°ì´í„°ì…‹')
    parser.add_argument('--src_lang', type=str, default='en', help='ì†ŒìŠ¤ ì–¸ì–´ ì½”ë“œ')
    parser.add_argument('--tgt_lang', type=str, default='ko', help='íƒ€ê²Ÿ ì–¸ì–´ ì½”ë“œ')
    parser.add_argument('--num_samples', type=int, default=10000, help='ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--tokenizer', type=str, default='bert-base-multilingual-cased',
                       help='ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €')
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument('--hidden_size', type=int, default=256, help='ìˆ¨ê¹€ ì°¨ì›')
    parser.add_argument('--encoder_layers', type=int, default=4, help='ì¸ì½”ë” ë ˆì´ì–´ ìˆ˜')
    parser.add_argument('--decoder_layers', type=int, default=4, help='ë””ì½”ë” ë ˆì´ì–´ ìˆ˜')
    parser.add_argument('--attention_heads', type=int, default=8, help='ì–´í…ì…˜ í—¤ë“œ ìˆ˜')
    parser.add_argument('--max_length', type=int, default=128, help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´')
    
    # í›ˆë ¨ ì„¤ì •
    parser.add_argument('--epochs', type=int, default=5, help='ì—í¬í¬ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=4, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=4, 
                       help='ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='ê°€ì¤‘ì¹˜ ê°ì‡ ')
    parser.add_argument('--save_dir', type=str, default='encoder_decoder_checkpoints_hf',
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('mps' if torch.backends.mps.is_available() 
                         else 'cuda' if torch.cuda.is_available() 
                         else 'cpu')
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)
    print(f"ğŸ“ í† í¬ë‚˜ì´ì €: {args.tokenizer}")
    
    # ëª¨ë¸ ì„¤ì •
    config = EncoderDecoderConfig(
        vocab_size=tokenizer.vocab_size,
        hidden_size=args.hidden_size,
        encoder_layers=args.encoder_layers,
        decoder_layers=args.decoder_layers,
        encoder_attention_heads=args.attention_heads,
        decoder_attention_heads=args.attention_heads,
        max_position_embeddings=args.max_length,
        pad_token_id=tokenizer.pad_token_id,
        bos_token_id=tokenizer.cls_token_id,
        eos_token_id=tokenizer.sep_token_id,
    )
    
    # ëª¨ë¸ ìƒì„±
    model = BERTEncoderTransformerDecoderModel(config)
    print(f"ğŸ¤– Encoder-Decoder ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“š ë²ˆì—­ ë°ì´í„°ì…‹ ë¡œë“œ: {args.dataset} ({args.src_lang} â†’ {args.tgt_lang})")
    
    try:
        train_dataset = HuggingFaceTranslationDataset(
            dataset_name=args.dataset,
            src_lang=args.src_lang,
            tgt_lang=args.tgt_lang,
            tokenizer_name=args.tokenizer,
            max_length=args.max_length,
            num_samples=args.num_samples,
            split="train"
        )
        
        # ê²€ì¦ ë°ì´í„°ì…‹
        val_dataset = None
        try:
            val_dataset = HuggingFaceTranslationDataset(
                dataset_name=args.dataset,
                src_lang=args.src_lang,
                tgt_lang=args.tgt_lang,
                tokenizer_name=args.tokenizer,
                max_length=args.max_length,
                num_samples=min(1000, args.num_samples // 10),
                split="validation"
            )
            print(f"ğŸ“š ê²€ì¦ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(val_dataset)}ê°œ")
        except:
            print("âš ï¸  ê²€ì¦ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê²€ì¦ ì—†ì´ ì§„í–‰")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ëŒ€ì•ˆ: ë‹¤ë¥¸ ì–¸ì–´ ìŒì´ë‚˜ ë°ì´í„°ì…‹ì„ ì‹œë„í•´ë³´ì„¸ìš”")
        print("   ì˜ˆ: --dataset opus100 --src_lang en --tgt_lang fr")
        return
    
    # í›ˆë ¨ê¸° ìƒì„±
    trainer = EncoderDecoderTrainerHF(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        device=device
    )
    
    # í›ˆë ¨ ì „ ë²ˆì—­ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” í›ˆë ¨ ì „ ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
    if args.src_lang == "en" and args.tgt_lang == "ko":
        test_text = "Hello, how are you today?"
    elif args.src_lang == "en" and args.tgt_lang == "fr":
        test_text = "The weather is beautiful today."
    else:
        test_text = "Good morning, how are you?"
    
    translation = trainer.translate_sample(test_text)
    print(f"   ì›ë¬¸: {test_text}")
    print(f"   ë²ˆì—­: {translation}")
    
    # í›ˆë ¨ ì‹¤í–‰
    results = trainer.train(args.epochs, args.save_dir)
    
    # í›ˆë ¨ í›„ ë²ˆì—­ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” í›ˆë ¨ í›„ ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
    translation = trainer.translate_sample(test_text)
    print(f"   ì›ë¬¸: {test_text}")
    print(f"   ë²ˆì—­: {translation}")
    
    # ë‹¤ì–‘í•œ ë¬¸ì¥ ë²ˆì—­ í…ŒìŠ¤íŠ¸
    if args.src_lang == "en":
        test_sentences = [
            "Good morning!",
            "Thank you very much.",
            "I love this book.",
            "The weather is nice today."
        ]
    else:
        test_sentences = [
            "Hello world",
            "How are you?",
            "Nice to meet you.",
            "Have a good day."
        ]
    
    print(f"\nğŸ§ª ë‹¤ì–‘í•œ ë¬¸ì¥ ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
    for sentence in test_sentences:
        translation = trainer.translate_sample(sentence)
        print(f"   {sentence} â†’ {translation}")
    
    # ê²°ê³¼ ì €ì¥
    results_path = os.path.join(args.save_dir, "training_results.json")
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ í›ˆë ¨ ê²°ê³¼ ì €ì¥: {results_path}")
    print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {args.save_dir}/")


if __name__ == "__main__":
    main()