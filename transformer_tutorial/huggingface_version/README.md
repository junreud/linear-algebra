# HuggingFace Version - Transformer Tutorial

ì´ ë””ë ‰í† ë¦¬ëŠ” HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì‹¤ì œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ê³ ê¸‰ Transformer êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

### 1. BERT Encoder + Transformer Decoder í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
- **BERTì˜ ì–‘ë°©í–¥ ì¸ì½”ë”**: ì†ŒìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ì–‘ë°©í–¥ìœ¼ë¡œ ì¸ì½”ë”©
- **Transformerì˜ ì¸ê³¼ì  ë””ì½”ë”**: íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±
- **í¬ë¡œìŠ¤ ì–´í…ì…˜**: ì¸ì½”ë”ì™€ ë””ì½”ë” ê°„ ì •ë³´ ì „ë‹¬
- **ì‹¤ì œ ë²ˆì—­ ì‘ì—…**: T5, BARTì™€ ìœ ì‚¬í•œ êµ¬ì¡°

### 2. ì‹¤ì œ ë°ì´í„°ì…‹ ì§€ì›
- **HuggingFace Datasets**: ì‹¤ì œ ë²ˆì—­ ë°ì´í„°ì…‹ ì‚¬ìš©
- **ë‹¤ì–‘í•œ ì–¸ì–´ ìŒ**: EN-KO, EN-FR, EN-DE ë“±
- **ê³ í’ˆì§ˆ ë°ì´í„°**: OPUS Books, WMT, OPUS-100

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
huggingface_version/
â”œâ”€â”€ README.md                        # ì´ íŒŒì¼
â”œâ”€â”€ encoder_decoder_model.py         # BERT+Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
â”œâ”€â”€ train_encoder_decoder_hf.py      # HuggingFace ë°ì´í„°ì…‹ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ custom_trainer.py               # ì»¤ìŠ¤í…€ í›ˆë ¨ê¸°
â”œâ”€â”€ model.py                        # ê¸°ë³¸ ëª¨ë¸ë“¤
â””â”€â”€ train.py                        # ê¸°ë³¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
```

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### BERT Encoder + Transformer Decoder

```
ì…ë ¥ í…ìŠ¤íŠ¸ (ì†ŒìŠ¤ ì–¸ì–´)
    â†“
ğŸ”„ BERT Encoder (ì–‘ë°©í–¥)
- TrackedTransformerLayer Ã— N
- Self-attention (bidirectional)
- Position embedding
    â†“
ì¸ì½”ë” ì¶œë ¥ (hidden states)
    â†“
ğŸ¯ Transformer Decoder (ë‹¨ë°©í–¥)
- DecoderLayer Ã— N  
- Masked self-attention (causal)
- Cross-attention (encoder â†’ decoder)
- Feed forward
    â†“
ì¶œë ¥ í…ìŠ¤íŠ¸ (íƒ€ê²Ÿ ì–¸ì–´)
```

### í¬ë¡œìŠ¤ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜

```python
# Query: ë””ì½”ë”ì˜ í˜„ì¬ ìƒíƒœ
# Key, Value: ì¸ì½”ë”ì˜ ì¶œë ¥
cross_attention_output = CrossAttention(
    query=decoder_hidden,      # ë””ì½”ë”ì—ì„œ
    key=encoder_hidden,        # ì¸ì½”ë”ì—ì„œ  
    value=encoder_hidden       # ì¸ì½”ë”ì—ì„œ
)
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

```bash
cd transformer_tutorial
source transformer_env/bin/activate

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install datasets transformers nltk
```

### 2. ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸

```bash
cd huggingface_version
python test_encoder_decoder.py
```

### 3. HuggingFace ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨

#### ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (OPUS Books)
```bash
python train_encoder_decoder_hf.py \
    --dataset opus_books \
    --src_lang en --tgt_lang ko \
    --epochs 5 --num_samples 5000 \
    --batch_size 4 --gradient_accumulation_steps 4
```

#### ì˜ì–´ â†’ í”„ë‘ìŠ¤ì–´ ë²ˆì—­ (WMT16)
```bash
python train_encoder_decoder_hf.py \
    --dataset wmt16 \
    --src_lang en --tgt_lang fr \
    --epochs 10 --batch_size 8 \
    --hidden_size 512 --encoder_layers 6
```

#### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì‘ì€ ëª¨ë¸)
```bash
python train_encoder_decoder_hf.py \
    --dataset opus100 \
    --src_lang en --tgt_lang de \
    --num_samples 1000 --epochs 2 \
    --hidden_size 128 --encoder_layers 2 --decoder_layers 2
```

### 4. HuggingFace í›ˆë ¨ í…ŒìŠ¤íŠ¸
```bash
python test_hf_training.py
```

## ğŸ“Š ì§€ì› ë°ì´í„°ì…‹

### ë²ˆì—­ ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹ | ì„¤ëª… | ì–¸ì–´ ìŒ ì˜ˆì‹œ | í’ˆì§ˆ |
|---------|------|-------------|------|
| `opus_books` | ë‹¤êµ­ì–´ ë„ì„œ ë²ˆì—­ | EN-KO, EN-FR, EN-DE | â­â­â­â­â­ |
| `wmt16` | WMT 2016 ë²ˆì—­ ëŒ€íšŒ | EN-FR, EN-DE, EN-RU | â­â­â­â­â­ |
| `wmt14` | WMT 2014 ë²ˆì—­ ëŒ€íšŒ | EN-FR, EN-DE | â­â­â­â­â­ |
| `opus100` | 100ê°œ ì–¸ì–´ ë²ˆì—­ | EN-XX (100+ ì–¸ì–´) | â­â­â­â­ |
| `kde4` | KDE ì†Œí”„íŠ¸ì›¨ì–´ ë²ˆì—­ | ë‹¤ì–‘í•œ ì–¸ì–´ ìŒ | â­â­â­ |

### ë°ì´í„°ì…‹ ì„ íƒ ê°€ì´ë“œ

- **ë†’ì€ í’ˆì§ˆ**: `opus_books`, `wmt16`, `wmt14`
- **ë‹¤ì–‘í•œ ì–¸ì–´**: `opus100`
- **ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: `kde4`
- **í•œêµ­ì–´ í¬í•¨**: `opus_books`, `opus100`

## ğŸ”§ ëª¨ë¸ ì„¤ì •

### ê¸°ë³¸ ì„¤ì • (33M íŒŒë¼ë¯¸í„°)
```python
config = EncoderDecoderConfig(
    hidden_size=256,
    encoder_layers=4,
    decoder_layers=4,
    encoder_attention_heads=8,
    decoder_attention_heads=8,
    max_position_embeddings=128
)
```

### ëŒ€í˜• ì„¤ì • (100M+ íŒŒë¼ë¯¸í„°)
```python
config = EncoderDecoderConfig(
    hidden_size=512,
    encoder_layers=8,
    decoder_layers=8,
    encoder_attention_heads=16,
    decoder_attention_heads=16,
    max_position_embeddings=256
)
```

## ğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì˜ˆì‹œ

```
ğŸš€ Encoder-Decoder ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (HuggingFace ë°ì´í„°)
ğŸ“Š í›ˆë ¨ ì„¤ì •:
   - ì—í¬í¬: 5
   - ë°°ì¹˜ í¬ê¸°: 4  
   - Gradient Accumulation: 4
   - ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: 16
   - í•™ìŠµë¥ : 0.0001
   - ë””ë°”ì´ìŠ¤: mps
   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: 33,660,929

ğŸ“ˆ Epoch 1/5
Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [03:45<00:00,  1.39it/s]
âœ… Train Loss: 8.2451, BLEU: 0.0234
âœ… Val Loss: 7.8932, BLEU: 0.0389

ğŸ” ë²ˆì—­ ìƒ˜í”Œ:
   1. ì˜ˆì¸¡: Bonjour comment allez vous aujourd hui
      ì •ë‹µ: Hello, how are you today?
```

## ğŸ” ë²ˆì—­ í…ŒìŠ¤íŠ¸

í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë²ˆì—­ í…ŒìŠ¤íŠ¸:

```python
from train_encoder_decoder_hf import EncoderDecoderTrainerHF
import torch

# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
checkpoint = torch.load("encoder_decoder_checkpoints_hf/best_model.pt")
model.load_state_dict(checkpoint['model_state_dict'])

# ë²ˆì—­ ì‹¤í–‰
trainer = EncoderDecoderTrainerHF(model, tokenizer, ...)
translation = trainer.translate_sample("Hello, how are you?")
print(f"ë²ˆì—­ ê²°ê³¼: {translation}")
```

## ğŸ†š ê¸°ì¡´ ë²„ì „ê³¼ì˜ ì°¨ì´ì 

### ê¸°ì¡´ ë²„ì „ (`train_encoder_decoder.py`)
- âŒ í•˜ë“œì½”ë”©ëœ 20ê°œ ë²ˆì—­ ìŒ
- âŒ ì œí•œì ì¸ í•™ìŠµ ë°ì´í„°
- âŒ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€ ì–´ë ¤ì›€

### HuggingFace ë²„ì „ (`train_encoder_decoder_hf.py`)
- âœ… ì‹¤ì œ ëŒ€ê·œëª¨ ë²ˆì—­ ë°ì´í„°ì…‹
- âœ… ë‹¤ì–‘í•œ ì–¸ì–´ ìŒ ì§€ì›
- âœ… BLEU ì ìˆ˜ í‰ê°€
- âœ… ì²´ê³„ì ì¸ ê²€ì¦ ë°ì´í„°
- âœ… í’ˆì§ˆ ì¢‹ì€ ë²ˆì—­ ê²°ê³¼

## ğŸ¯ ì„±ëŠ¥ ê°œì„  íŒ

### 1. ë°ì´í„° í’ˆì§ˆ
- ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ì„ íƒ (`opus_books`, `wmt16`)
- ì¶©ë¶„í•œ ë°ì´í„° ì–‘ (10K+ ìƒ˜í”Œ)
- ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ í•„í„°ë§

### 2. ëª¨ë¸ í¬ê¸°
- ì‘ì€ ëª¨ë¸: 128-256 hidden size (ë¹ ë¥¸ ì‹¤í—˜)
- ì¤‘ê°„ ëª¨ë¸: 512 hidden size (ê· í˜•ì¡íŒ ì„±ëŠ¥)
- í° ëª¨ë¸: 768+ hidden size (ìµœê³  ì„±ëŠ¥)

### 3. í›ˆë ¨ ì„¤ì •
- Learning rate: 1e-4 ~ 5e-4
- Batch size: ì‹¤ì œ ë°°ì¹˜ í¬ê¸° 16-32 (gradient accumulation í™œìš©)
- Epochs: ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ 5-20

### 4. í‰ê°€ ì§€í‘œ
- BLEU ì ìˆ˜: ë²ˆì—­ í’ˆì§ˆ ì¸¡ì •
- ì†ì‹¤ ê°’: í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§
- ìƒ˜í”Œ ë²ˆì—­: ì •ì„±ì  í‰ê°€

## ğŸš€ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¶”ê°€
```python
class CustomTranslationDataset(Dataset):
    def __init__(self, src_texts, tgt_texts, tokenizer):
        # ì»¤ìŠ¤í…€ ë°ì´í„° ë¡œë”© ë¡œì§
        pass
```

### 2. ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €
```python
# ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ì§€ì›
tokenizers = {
    "multilingual": "bert-base-multilingual-cased",
    "korean": "klue/bert-base",
    "french": "dbmdz/bert-base-french-europeana-cased",
    "german": "bert-base-german-cased"
}
```

### 3. ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ
```python
# í›ˆë ¨ ì¬ê°œ
checkpoint = torch.load("encoder_decoder_checkpoints_hf/checkpoint_epoch_3.pt")
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- **BERT**: Bidirectional Encoder Representations from Transformers
- **Transformer**: Attention Is All You Need
- **T5**: Text-to-Text Transfer Transformer  
- **BART**: Denoising Sequence-to-Sequence Pre-training
- **HuggingFace Datasets**: ğŸ¤— Datasets Library

---

ğŸ’¡ **íŒ**: ì‹¤ì œ ë²ˆì—­ ëª¨ë¸ ê°œë°œì—ì„œëŠ” ì´ëŸ¬í•œ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ê°€ ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤. BERTì˜ ê°•ë ¥í•œ ì¸ì½”ë”© ëŠ¥ë ¥ê³¼ Transformerì˜ ìƒì„± ëŠ¥ë ¥ì„ ê²°í•©í•˜ì—¬ ì‹¤ìš©ì ì¸ ë²ˆì—­ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!