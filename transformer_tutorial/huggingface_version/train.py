"""
Hugging Face ë²„ì „ Transformer í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì´ íŒŒì¼ì€ Hugging Face ìƒíƒœê³„ë¥¼ í™œìš©í•œ ì‹¤ë¬´í˜• Transformer í•™ìŠµì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
PyTorch ìˆœì • êµ¬í˜„ê³¼ ë‹¬ë¦¬, ë‹¤ìŒê³¼ ê°™ì€ ì‹¤ë¬´ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:

ì£¼ìš” íŠ¹ì§•:
1. ì‹¤ì œ ë°ì´í„°ì…‹ ì‚¬ìš© - wikitext, imdb ë“± Hugging Face Hub ë°ì´í„°ì…‹
2. ì‹¤ì œ í† í¬ë‚˜ì´ì € ì‚¬ìš© - GPT-2, BERT ë“± ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €
3. Hugging Face Trainer ì‚¬ìš© - ìë™ ìµœì í™”, ë¡œê¹…, ì²´í¬í¬ì¸íŠ¸ ì €ì¥
4. WandB í†µí•© - ì‹¤í—˜ ì¶”ì  ë° ì‹œê°í™”
5. Mixed Precision - ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
6. ë‚´ë¶€ ìƒíƒœ ì¶”ì  - QKV, attention weights ë“± ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""
import torch
import torch.nn as nn
from transformers import AutoTokenizer, DataCollatorForLanguageModeling
from datasets import Dataset, load_dataset
import argparse
import os
import wandb
from typing import List, Dict
import numpy as np

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤ ê°€ì ¸ì˜¤ê¸°
from huggingface_version.model import create_tracked_transformer
from huggingface_version.custom_trainer import (
    CustomTransformerTrainer,
    create_training_arguments,
    SimpleTokenDataset
)


def load_real_dataset(
    dataset_name: str = "wikitext", 
    dataset_config: str = "wikitext-2-raw-v1",
    num_samples: int = 10000,
    split: str = "train"
) -> Dataset:
    """
    Hugging Face Hubì—ì„œ ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë”©
    
    ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ ('wikitext', 'imdb', 'ag_news', 'amazon_polarity' ë“±)
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì˜ˆ: 'wikitext-2-raw-v1', 'wikitext-103-raw-v1')
        num_samples: ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜ (ì „ì²´ ë°ì´í„°ì…‹ì´ í´ ê²½ìš° ì œí•œ)
        split: ë°ì´í„° ë¶„í•  ('train', 'validation', 'test')
    
    Returns:
        HuggingFace Dataset ê°ì²´
    
    ì§€ì›í•˜ëŠ” ë°ì´í„°ì…‹ë“¤:
    1. wikitext: ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ (ì–¸ì–´ ëª¨ë¸ë§ìš©)
    2. imdb: ì˜í™” ë¦¬ë·° (ê°ì • ë¶„ì„ìš©, í…ìŠ¤íŠ¸ë¡œë„ ì‚¬ìš© ê°€ëŠ¥)
    3. ag_news: ë‰´ìŠ¤ ë¶„ë¥˜ ë°ì´í„°
    4. amazon_polarity: ì•„ë§ˆì¡´ ë¦¬ë·° ê°ì • ë¶„ì„
    5. bookcorpus: ì±… í…ìŠ¤íŠ¸ (ëŒ€ìš©ëŸ‰)
    """
    
    print(f"ğŸ“š ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {dataset_name} ({dataset_config})")
    print(f"   ë¶„í• : {split}, ìµœëŒ€ ìƒ˜í”Œ ìˆ˜: {num_samples:,}")
    
    try:
        # Hugging Face Hubì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ
        if dataset_config:
            dataset = load_dataset(dataset_name, dataset_config, split=split)
        else:
            dataset = load_dataset(dataset_name, split=split)
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset):,} ìƒ˜í”Œ")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ë° í›ˆë ¨ ì‹œê°„ ê³ ë ¤)
        if len(dataset) > num_samples:
            dataset = dataset.select(range(num_samples))
            print(f"ğŸ“ ìƒ˜í”Œ ìˆ˜ ì œí•œ: {len(dataset):,} ìƒ˜í”Œ ì‚¬ìš©")
        
        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸ ë° í…ìŠ¤íŠ¸ í•„ë“œ í†µì¼
        print(f"ğŸ“‹ ë°ì´í„°ì…‹ ì»¬ëŸ¼: {list(dataset.features.keys())}")
        
        # í…ìŠ¤íŠ¸ í•„ë“œ ì°¾ê¸° ë° 'text'ë¡œ í†µì¼
        text_field = None
        if 'text' in dataset.features:
            text_field = 'text'
        elif 'sentence' in dataset.features:
            text_field = 'sentence'
        elif 'review' in dataset.features:
            text_field = 'review'
        elif 'content' in dataset.features:
            text_field = 'content'
        else:
            # ì²« ë²ˆì§¸ ë¬¸ìì—´ í•„ë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            for key, feature in dataset.features.items():
                if feature.dtype == 'string':
                    text_field = key
                    break
        
        if text_field is None:
            raise ValueError(f"í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ: {list(dataset.features.keys())}")
        
        if text_field != 'text':
            dataset = dataset.rename_column(text_field, 'text')
            print(f"ğŸ”„ í…ìŠ¤íŠ¸ í•„ë“œ '{text_field}' â†’ 'text'ë¡œ ì´ë¦„ ë³€ê²½")
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
        original_len = len(dataset)
        dataset = dataset.filter(lambda x: x['text'] and len(x['text'].strip()) > 10)
        print(f"ğŸ§¹ ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°: {original_len:,} â†’ {len(dataset):,} ìƒ˜í”Œ")
        
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¶œë ¥
        print(f"\nğŸ“„ ìƒ˜í”Œ í…ìŠ¤íŠ¸ (ì²˜ìŒ 3ê°œ):")
        for i in range(min(3, len(dataset))):
            text = dataset[i]['text'][:100]  # ì²˜ìŒ 100ìë§Œ
            print(f"   {i+1}. {text}{'...' if len(dataset[i]['text']) > 100 else ''}")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
        print(f"ğŸ”„ ê¸°ë³¸ synthetic ë°ì´í„°ì…‹ìœ¼ë¡œ fallback")
        return create_synthetic_dataset(num_samples)


def create_synthetic_dataset(num_samples: int = 10000) -> Dataset:
    """
    ê°„ë‹¨í•œ synthetic ë°ì´í„°ì…‹ ìƒì„± (fallbackìš©)
    
    ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë”©ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë°±ì—… í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    
    # ê°„ë‹¨í•œ í…œí”Œë¦¿ ë¬¸ì¥ë“¤
    templates = [
        "The {animal} {verb} in the {location} during {time}.",
        "A {adjective} {object} was {verb} by the {person}.",
        "In the {location}, {person} {verb} {adjective} {object}.",
        "During {time}, the {animal} {verb} {adjective}ly.",
        "The {person} saw a {adjective} {animal} near the {location}.",
        "Yesterday, {person} visited the {location} and saw many {animal}s.",
        "The {adjective} {object} belongs to {person} who lives in {location}.",
        "Every {time}, {animal} {verb} near the old {object} in the {location}."
    ]
    
    # ë‹¨ì–´ ëª©ë¡
    words = {
        'animal': ['cat', 'dog', 'bird', 'fish', 'horse', 'tiger', 'lion', 'elephant', 'rabbit', 'deer'],
        'verb': ['runs', 'jumps', 'flies', 'swims', 'walks', 'sleeps', 'eats', 'plays', 'sits', 'stands'],
        'location': ['park', 'forest', 'beach', 'mountain', 'garden', 'lake', 'field', 'city', 'village', 'river'],
        'time': ['morning', 'evening', 'night', 'afternoon', 'dawn', 'dusk', 'midnight', 'noon', 'sunrise', 'sunset'],
        'adjective': ['big', 'small', 'beautiful', 'fast', 'slow', 'bright', 'dark', 'quiet', 'loud', 'colorful'],
        'object': ['book', 'car', 'house', 'tree', 'flower', 'stone', 'bridge', 'chair', 'table', 'window'],
        'person': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry', 'Iris', 'Jack']
    }
    
    import random
    
    texts = []
    for _ in range(num_samples):
        template = random.choice(templates)
        text = template
        for placeholder, word_list in words.items():
            if f'{{{placeholder}}}' in text:
                text = text.replace(f'{{{placeholder}}}', random.choice(word_list))
        texts.append(text)
    
    dataset = Dataset.from_dict({'text': texts})
    return dataset


def create_tokenizer(model_name: str = "gpt2", vocab_size: int = None):
    """
    ì‹¤ì œ ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ìƒì„±
    í† í¬ë‚˜ì´ì €ê°€ ì˜ í›ˆë ¨ë˜ì—ˆê³  ì•„ë‹ˆê³ ì˜ ì°¨ì´ëŠ” 
        ê°„ë‹¨: ë‹¨ì–´ ë‹¨ìœ„ë¡œë§Œ ë¶„í• 
        í›ˆë ¨ëœ: ì˜ë¯¸ ìˆëŠ” ì„œë¸Œì›Œë“œë¡œ ë¶„í• 
        "tokenization" â†’ ["token", "ization"]
        "tokenizer" â†’ ["token", "izer"]
        "tokenizing" â†’ ["token", "izing"]
    ì´ëŸ°ì‹ìœ¼ë¡œ ë‹¨ì–´ì˜ íŒŒìƒëœ ë¶€ë¶„ì„ ì´í•´í•˜ëƒ ëª»í•˜ëƒê°€ ë  ìˆ˜ë„ ìˆê³ , ê¸°í˜¸ê°€ ë“¤ì–´ê°„ ë¬¸ì¥ ë“±ë„ ì˜ êµ¬ë¶„í•˜ê²Œë¨
    ì´ í† í¬ë‚˜ì´ì €ì˜ í•™ìŠµì€ ì¼ë°˜ì ì¸ ë”¥ëŸ¬ë‹ í›ˆë ¨ê³¼ëŠ” ì•„ì˜ˆ ë‹¤ë¦„.
    ë‹¨ìˆœ í†µê³„ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¯¸ë¶„, ë¡œìŠ¤ìœ¨ ê³„ì‚°ì´ ì—†ì´ ë‹¤ëŸ‰ì˜ ë°ì´í„°(ë¼ë²¨ë°ì´í„° í•„ìš”ì—†ì‘)ë§Œìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥.

    Args:
        model_name: ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € ëª¨ë¸ ('gpt2', 'bert-base-uncased', 'distilbert-base-uncased' ë“±)
        vocab_size: ì–´íœ˜ ì‚¬ì „ í¬ê¸° (Noneì´ë©´ ì›ë³¸ í¬ê¸° ì‚¬ìš©)
    
    Returns:
        AutoTokenizer ê°ì²´
    
    ì§€ì›í•˜ëŠ” í† í¬ë‚˜ì´ì €ë“¤:
    1. gpt2: GPT-2 í† í¬ë‚˜ì´ì € (BPE ê¸°ë°˜)
    2. bert-base-uncased: BERT í† í¬ë‚˜ì´ì € (WordPiece ê¸°ë°˜)
    3. distilbert-base-uncased: DistilBERT í† í¬ë‚˜ì´ì €
    4. t5-small: T5 í† í¬ë‚˜ì´ì € (SentencePiece ê¸°ë°˜)
    """
    
    print(f"ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘: {model_name}")
    
    try:
        # Hugging Faceì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì • (GPT-2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íŒ¨ë”© í† í°ì´ ì—†ìŒ)
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
                print(f"ğŸ“ íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •: '{tokenizer.eos_token}'")
            else:
                tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                print(f"ğŸ“ ìƒˆë¡œìš´ íŒ¨ë”© í† í° ì¶”ê°€: '[PAD]'")
        
        print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ:")
        print(f"   ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer):,}")
        print(f"   íŒ¨ë”© í† í°: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")
        
        # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
        test_text = "Hello, this is a test sentence for tokenization."
        test_tokens = tokenizer.tokenize(test_text)
        test_ids = tokenizer.encode(test_text)
        
        print(f"\nğŸ§ª í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸:")
        print(f"   ì…ë ¥: '{test_text}'")
        print(f"   í† í°: {test_tokens[:10]}{'...' if len(test_tokens) > 10 else ''}")
        print(f"   ID: {test_ids[:10]}{'...' if len(test_ids) > 10 else ''}")
        
        return tokenizer
        
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
        print(f"ğŸ”„ ê¸°ë³¸ ê°„ë‹¨ í† í¬ë‚˜ì´ì €ë¡œ fallback")
        return create_simple_tokenizer(vocab_size or 1000)


class SimpleTokenizer: # ë³´í—˜ìš©, ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜, ì¸í„°ë„· ì—°ê²°ë¬¸ì œ ë“± ë°œìƒ ì‹œ  ì‹¤í–‰
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € (fallbackìš©)"""
    
    def __init__(self, vocab_size: int = 1000):
        self.vocab_size = vocab_size
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        
        self.pad_token = '<pad>'
        self.unk_token = '<unk>'
        self.bos_token = '<bos>'
        self.eos_token = '<eos>'
        
        # ê°„ë‹¨í•œ vocab êµ¬ì¶•
        self.vocab = {
            '<pad>': self.pad_token_id,
            '<unk>': self.unk_token_id,
            '<bos>': self.bos_token_id,
            '<eos>': self.eos_token_id,
        }
        
        # ì¶”ê°€ í† í°ë“¤
        common_words = [
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after',
            'above', 'below', 'between', 'among', 'under', 'over', 'is', 'was', 'are', 'were',
            'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'must', 'can', 'cat', 'dog', 'bird', 'fish',
            'runs', 'jumps', 'flies', 'swims', 'walks', 'park', 'forest', 'beach', 'big', 'small'
        ]
        
        for word in common_words:
            if len(self.vocab) < vocab_size:
                self.vocab[word] = len(self.vocab)
        
        # ì—­ë°©í–¥ ë§¤í•‘
        self.id_to_token = {v: k for k, v in self.vocab.items()}
    
    def __len__(self):
        return len(self.vocab)
    
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• """
        import re
        tokens = re.findall(r'\w+|[^\w\s]', text.lower())
        return tokens
    
    def encode(self, text: str, add_special_tokens: bool = True, max_length: int = None, 
               padding: str = None, truncation: bool = False, return_tensors: str = None):
        """í…ìŠ¤íŠ¸ ì¸ì½”ë”©"""
        tokens = self.tokenize(text)
        
        if add_special_tokens:
            tokens = ['<bos>'] + tokens + ['<eos>']
        
        token_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]
        
        # Truncation
        if truncation and max_length and len(token_ids) > max_length:
            token_ids = token_ids[:max_length]
        
        # Attention mask
        attention_mask = [1] * len(token_ids)
        
        # Padding
        if padding == 'max_length' and max_length:
            if len(token_ids) < max_length:
                pad_length = max_length - len(token_ids)
                token_ids.extend([self.pad_token_id] * pad_length)
                attention_mask.extend([0] * pad_length)
        
        result = {
            'input_ids': token_ids,
            'attention_mask': attention_mask
        }
        
        if return_tensors == 'pt':
            result = {k: torch.tensor([v]) for k, v in result.items()}
        
        return result
    
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        tokens = []
        for token_id in token_ids:
            token = self.id_to_token.get(token_id, '<unk>')
            if skip_special_tokens and token in ['<pad>', '<bos>', '<eos>', '<unk>']:
                continue
            tokens.append(token)
        return ' '.join(tokens)


def create_simple_tokenizer(vocab_size: int = 1000):
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € ìƒì„± (fallbackìš©)"""
    return SimpleTokenizer(vocab_size)


def preprocess_dataset(dataset: Dataset, tokenizer, max_length: int = 128, num_proc: int = 4):
    """
    ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ - í† í¬ë‚˜ì´ì œì´ì…˜ ë° íŒ¨ë”©
    
    Args:
        dataset: ì›ë³¸ ë°ì´í„°ì…‹
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        num_proc: ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
    
    Returns:
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹
    """
    
    def tokenize_function(examples):
        """ë°°ì¹˜ í† í¬ë‚˜ì´ì œì´ì…˜ í•¨ìˆ˜"""
        # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆ
        if hasattr(tokenizer, 'encode_batch'):  # ì‹¤ì œ HF í† í¬ë‚˜ì´ì €
            return tokenizer(
                examples['text'],       # ë¦¬ìŠ¤íŠ¸ ì…ë ¥
                truncation=True,        # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ìë¥´ê¸°
                padding='max_length',   # ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”©
                max_length=max_length,  # ìµœëŒ€ ê¸¸ì´ ì„¤ì •
                return_tensors=None     # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            )
        else:  # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €
            results = {'input_ids': [], 'attention_mask': []}
            for text in examples['text']:
                encoded = tokenizer.encode(
                    text,
                    add_special_tokens=True,
                    max_length=max_length,
                    padding='max_length',
                    truncation=True
                )
                results['input_ids'].append(encoded['input_ids'])
                results['attention_mask'].append(encoded['attention_mask'])
            return results
    
    print(f"ğŸ”„ ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì œì´ì…˜ ì¤‘...")
    print(f"   ìµœëŒ€ ê¸¸ì´: {max_length}")
    print(f"   ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤: {num_proc}")
    
    # í† í¬ë‚˜ì´ì œì´ì…˜ ì‹¤í–‰
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,                       # ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬
        num_proc=num_proc,                  # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        remove_columns=dataset.column_names,  # ì›ë³¸ í…ìŠ¤íŠ¸ ì œê±°í•¨
        desc="í† í¬ë‚˜ì´ì œì´ì…˜ ì§„í–‰ ì¤‘"
    )
    """
    í…ìŠ¤íŠ¸: ["Hello world", "This is a test", "Short"]
    í† í¬ë‚˜ì´ì œì´ì…˜ í›„:
        {
            'input_ids': [
                [15496, 995, 0, 0, 0],     # "Hello world" + íŒ¨ë”©      íŒ¨ë”©ì´ ì¡´ì¬í•˜ëŠ” ì´ìœ : GPU/CPUëŠ” ê°™ì€ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
                [1212, 318, 257, 1332, 0], # "This is a test" + íŒ¨ë”©
                [17896, 0, 0, 0, 0]        # "Short" + íŒ¨ë”©
            ],
            'attention_mask': [
                [1, 1, 0, 0, 0],           # ì²˜ìŒ 2ê°œë§Œ ìœ íš¨
                [1, 1, 1, 1, 0],           # ì²˜ìŒ 4ê°œë§Œ ìœ íš¨
                [1, 0, 0, 0, 0]            # ì²˜ìŒ 1ê°œë§Œ ìœ íš¨
            ]
        }
    """
    # labels ì»¬ëŸ¼ ì¶”ê°€ (ì–¸ì–´ ëª¨ë¸ë§ìš© - input_idsì™€ ë™ì¼) ìœ„ì˜ í† í¬ë‚˜ì´ì œì´ì…˜ í›„ì˜ ë°ì´í„°ì— labels ì»¬ëŸ¼ì´ ì¶”ê°€ë¨. 
    # input_idsì™€ ë™ì¼í•œ ê°’ ê°€ì§. ëª¨ë¸ í›ˆë ¨ ì‹œ ë¡œìŠ¤ ê³„ì‚°ì— ì‚¬ìš©ë¨.
    def add_labels(examples):
        examples['labels'] = examples['input_ids'].copy()
        return examples
    
    tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)
    
    print(f"âœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ: {len(tokenized_dataset):,} ìƒ˜í”Œ")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“„ í† í¬ë‚˜ì´ì œì´ì…˜ ìƒ˜í”Œ:")
    sample = tokenized_dataset[0]
    print(f"   input_ids ê¸¸ì´: {len(sample['input_ids'])}")
    print(f"   input_ids: {sample['input_ids'][:20]}...")
    print(f"   attention_mask: {sample['attention_mask'][:20]}...")
    
    if hasattr(tokenizer, 'decode'): # ì‚¬ëŒì´ ì½ê¸° í¸í•˜ê²Œ. skip_special_tokens=True: íŒ¨ë”©, BOS, EOS í† í° ë¬´ì‹œ
        decoded = tokenizer.decode(sample['input_ids'][:50], skip_special_tokens=True) 
        print(f"   ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: '{decoded[:100]}...'")
    
    return tokenized_dataset


def setup_wandb(project_name: str = "transformer-tutorial", config: dict = None):
    """WandB ì„¤ì •"""
    try:
        # WandB ì´ˆê¸°í™”
        wandb.init(
            project=project_name,
            config=config,
            name=f"tracked-transformer-{wandb.util.generate_id()}"
        )
        print("âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Hugging Face Transformer í•™ìŠµ')
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument('--dataset_name', type=str, default='wikitext', 
                       help='ë°ì´í„°ì…‹ ì´ë¦„ (wikitext, imdb, ag_news ë“±)')
    parser.add_argument('--dataset_config', type=str, default='wikitext-2-raw-v1', 
                       help='ë°ì´í„°ì…‹ ì„¤ì •')
    parser.add_argument('--train_size', type=int, default=8000, help='í›ˆë ¨ ë°ì´í„° í¬ê¸°')
    parser.add_argument('--eval_size', type=int, default=2000, help='ê²€ì¦ ë°ì´í„° í¬ê¸°')
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    parser.add_argument('--tokenizer_name', type=str, default='gpt2', 
                       help='í† í¬ë‚˜ì´ì € ì´ë¦„ (gpt2, bert-base-uncased ë“±)')
    parser.add_argument('--max_length', type=int, default=128, help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´') # ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´, ë°°ì¹˜ ë‚´ì—ì„œ max_length ê¸¸ì´ë¡œ íŒ¨ë”©ë¨
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument('--vocab_size', type=int, default=None, help='ì–´íœ˜ ì‚¬ì „ í¬ê¸° (ìë™ ì¶”ì •)')
    parser.add_argument('--hidden_size', type=int, default=256, help='ìˆ¨ê¹€ ì°¨ì›')
    parser.add_argument('--num_layers', type=int, default=6, help='ë ˆì´ì–´ ìˆ˜')
    parser.add_argument('--num_heads', type=int, default=8, help='ì–´í…ì…˜ í—¤ë“œ ìˆ˜')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--epochs', type=int, default=3, help='ì—í­ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=16, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='í•™ìŠµë¥ ')
    parser.add_argument('--output_dir', type=str, default='./hf_results', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    # ì¶”ì  ë° ë¡œê¹… ì„¤ì •
    parser.add_argument('--track_internals', action='store_true', help='ë‚´ë¶€ ìƒíƒœ ì¶”ì ')
    parser.add_argument('--wandb', action='store_true', help='WandB ì‚¬ìš©')
    parser.add_argument('--wandb_project', type=str, default='hf-transformer-tutorial', 
                       help='WandB í”„ë¡œì íŠ¸ ì´ë¦„')
    
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print(f"HUGGING FACE TRANSFORMER í•™ìŠµ ì‹œì‘")
    print(f"{'='*80}")
    print(f"ğŸ“Š ì„¤ì •:")
    print(f"   ë°ì´í„°ì…‹: {args.dataset_name} ({args.dataset_config})")
    print(f"   í† í¬ë‚˜ì´ì €: {args.tokenizer_name}")
    print(f"   ëª¨ë¸: {args.hidden_size}d, {args.num_layers}L, {args.num_heads}H")
    print(f"   í•™ìŠµ: {args.epochs} epochs, lr={args.learning_rate}")
    print(f"   ë‚´ë¶€ ì¶”ì : {args.track_internals}")
    
    # 1. ë°ì´í„°ì…‹ ë¡œë”©
    print(f"\nğŸ“š 1ë‹¨ê³„: ë°ì´í„°ì…‹ ë¡œë”©")
    dataset = load_real_dataset(
        dataset_name=args.dataset_name,
        dataset_config=args.dataset_config,
        num_samples=args.train_size + args.eval_size,
        split="train"
    )
    
    # 2. í† í¬ë‚˜ì´ì € ìƒì„±
    print(f"\nğŸ”¤ 2ë‹¨ê³„: í† í¬ë‚˜ì´ì € ìƒì„±")
    tokenizer = create_tokenizer(args.tokenizer_name, args.vocab_size)
    
    # ì–´íœ˜ ì‚¬ì „ í¬ê¸° ìë™ ì„¤ì •
    if args.vocab_size is None:
        args.vocab_size = len(tokenizer)
        print(f"ğŸ“ ì–´íœ˜ ì‚¬ì „ í¬ê¸° ìë™ ì„¤ì •: {args.vocab_size:,}")
    
    # 3. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    print(f"\nğŸ”„ 3ë‹¨ê³„: ë°ì´í„°ì…‹ ì „ì²˜ë¦¬")
    tokenized_dataset = preprocess_dataset(dataset, tokenizer, args.max_length)
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í•  || ë³¸ì  ì—†ëŠ” ë°ì´í„°ë¡œ ê²€ì¦í•˜ê¸° ìœ„í•´ ë¶„í• . evaluation datasetì€ train datasetê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡
    train_dataset = tokenized_dataset.select(range(args.train_size))
    eval_dataset = tokenized_dataset.select(range(args.train_size, args.train_size + args.eval_size))
    
    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"   í›ˆë ¨: {len(train_dataset):,} ìƒ˜í”Œ")
    print(f"   ê²€ì¦: {len(eval_dataset):,} ìƒ˜í”Œ")
    
    # 4. ëª¨ë¸ ìƒì„±
    print(f"\nğŸ¤– 4ë‹¨ê³„: ëª¨ë¸ ìƒì„±")
    model = create_tracked_transformer(
        vocab_size=args.vocab_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        num_heads=args.num_heads,
        track_internal_states=args.track_internals
    )
    
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ:")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   í›ˆë ¨ ê°€ëŠ¥: {trainable_params:,}")
    
    # 5. WandB ì„¤ì • (ì„ íƒì‚¬í•­)
    if args.wandb:
        print(f"\nğŸ“Š 5ë‹¨ê³„: WandB ì„¤ì •")
        config = {
            'dataset_name': args.dataset_name,
            'tokenizer_name': args.tokenizer_name,
            'vocab_size': args.vocab_size,
            'hidden_size': args.hidden_size,
            'num_layers': args.num_layers,
            'num_heads': args.num_heads,
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'max_length': args.max_length,
            'total_params': total_params,
        }
        setup_wandb(args.wandb_project, config)
    
    # 6. í›ˆë ¨ ì¸ìˆ˜ ìƒì„±
    print(f"\nâš™ï¸ 6ë‹¨ê³„: í›ˆë ¨ ì„¤ì •")
    training_args = create_training_arguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        logging_steps=50,
        save_steps=500,
        eval_steps=500,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="wandb" if args.wandb else None
    )
    
    # 7. ë°ì´í„° ì½œë ˆì´í„° ìƒì„±
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer if hasattr(tokenizer, 'pad_token_id') else None,
        mlm=False,  # ìê¸°íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§ (GPT ìŠ¤íƒ€ì¼)
    )
    
    # 8. íŠ¸ë ˆì´ë„ˆ ìƒì„±
    print(f"\nğŸƒ 7ë‹¨ê³„: íŠ¸ë ˆì´ë„ˆ ìƒì„±")
    trainer = CustomTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        track_internal_states=args.track_internals
    )
    
    # 9. í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ 8ë‹¨ê³„: í•™ìŠµ ì‹œì‘")
    print(f"{'='*80}")
    
    trainer.train()
    
    # 10. ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ 9ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
    trainer.save_model()
    trainer.save_state()
    
    if hasattr(tokenizer, 'save_pretrained'):
        tokenizer.save_pretrained(args.output_dir)
        print(f"âœ… í† í¬ë‚˜ì´ì € ì €ì¥: {args.output_dir}")
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"   ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {args.output_dir}")
    print(f"   ì²´í¬í¬ì¸íŠ¸: {args.output_dir}/checkpoint-*")
    
    if args.wandb:
        wandb.finish()


if __name__ == "__main__":
    main()