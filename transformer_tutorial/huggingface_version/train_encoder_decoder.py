"""
BERT Encoder + Transformer Decoder ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Encoder-Decoder ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
ê°„ë‹¨í•œ ë²ˆì—­ ì‘ì—… (ì˜ì–´ â†’ í•œêµ­ì–´)ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•´ë´…ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python train_encoder_decoder.py
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch.nn.functional as F

from transformers import AutoTokenizer
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from encoder_decoder_model import BERTEncoderTransformerDecoderModel, EncoderDecoderConfig
import os
import json
from tqdm import tqdm
import numpy as np


class SimpleTranslationDataset(Dataset):
    """
    ê°„ë‹¨í•œ ë²ˆì—­ ë°ì´í„°ì…‹
    
    ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ì„ ìœ„í•œ ë”ë¯¸ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
    ì‹¤ì œë¡œëŠ” ë” í° ë³‘ë ¬ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ê°„ë‹¨í•œ ë²ˆì—­ ì˜ˆì‹œë“¤ (ì‹¤ì œë¡œëŠ” í° ë°ì´í„°ì…‹ ì‚¬ìš©)
        self.translation_pairs = [
            ("Hello world", "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"),
            ("How are you?", "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"),
            ("I love programming", "ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤"),
            ("The weather is nice", "ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤"),
            ("Good morning", "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤"),
            ("Thank you very much", "ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤"),
            ("See you later", "ë‚˜ì¤‘ì— ë´ìš”"),
            ("I am learning Korean", "ì €ëŠ” í•œêµ­ì–´ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤"),
            ("This is a beautiful day", "ì˜¤ëŠ˜ì€ ì•„ë¦„ë‹¤ìš´ ë‚ ì…ë‹ˆë‹¤"),
            ("Can you help me?", "ë„ì™€ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"),
            ("I like to read books", "ì €ëŠ” ì±… ì½ê¸°ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤"),
            ("The food is delicious", "ìŒì‹ì´ ë§›ìˆìŠµë‹ˆë‹¤"),
            ("Where is the library?", "ë„ì„œê´€ì´ ì–´ë””ì— ìˆë‚˜ìš”?"),
            ("I want to learn more", "ë” ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤"),
            ("Happy birthday", "ìƒì¼ ì¶•í•˜í•©ë‹ˆë‹¤"),
            ("What time is it?", "ëª‡ ì‹œì¸ê°€ìš”?"),
            ("I am from Korea", "ì €ëŠ” í•œêµ­ì—ì„œ ì™”ìŠµë‹ˆë‹¤"),
            ("Nice to meet you", "ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤"),
            ("Have a good day", "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”"),
            ("I understand", "ì´í•´í•©ë‹ˆë‹¤"),
        ]
        
        # ë°ì´í„° í™•ì¥ (í•™ìŠµìš©ìœ¼ë¡œ ë” ë§ì€ ì˜ˆì‹œ ìƒì„±)
        self.data = self.translation_pairs * 10  # 10ë°° ë³µì œ
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        english, korean = self.data[idx]
        
        # Encoder ì…ë ¥ (ì˜ì–´)
        encoder_inputs = self.tokenizer(
            english,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Decoder ì…ë ¥ ë° ë ˆì´ë¸” (í•œêµ­ì–´)
        # Teacher Forcing: ì…ë ¥ì€ [BOS] + í† í°ë“¤, ë ˆì´ë¸”ì€ í† í°ë“¤ + [EOS]
        korean_with_bos = f"[CLS] {korean}"  # BOS í† í° ì¶”ê°€
        korean_with_eos = f"{korean} [SEP]"  # EOS í† í° ì¶”ê°€
        
        decoder_inputs = self.tokenizer(
            korean_with_bos,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        labels = self.tokenizer(
            korean_with_eos,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoder_inputs['input_ids'].squeeze(0),
            'attention_mask': encoder_inputs['attention_mask'].squeeze(0),
            'decoder_input_ids': decoder_inputs['input_ids'].squeeze(0),
            'decoder_attention_mask': decoder_inputs['attention_mask'].squeeze(0),
            'labels': labels['input_ids'].squeeze(0),
        }


class EncoderDecoderTrainer:
    """
    Encoder-Decoder ëª¨ë¸ í•™ìŠµê¸°
    """
    
    def __init__(
        self, 
        model, 
        tokenizer, 
        train_dataset, 
        val_dataset=None,
        batch_size=8,
        learning_rate=5e-4,
        num_epochs=10,
        device='cpu'
    ):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        
        # ë°ì´í„° ë¡œë”
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0  # macOSì—ì„œ ì•ˆì •ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
        )
        
        self.val_loader = None
        if val_dataset:
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0
            )
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = AdamW(
            model.parameters(),
            lr=learning_rate,
            betas=(0.9, 0.98),
            eps=1e-9,
            weight_decay=0.01
        )
        
        total_steps = len(self.train_loader) * num_epochs
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=learning_rate * 0.1
        )
        
        self.num_epochs = num_epochs
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(self):
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(self.train_loader, desc="Training")
        
        for batch in progress_bar:
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # ìˆœì „íŒŒ
            outputs = self.model(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                decoder_input_ids=batch['decoder_input_ids'],
                decoder_attention_mask=batch['decoder_attention_mask'],
                labels=batch['labels'],
                return_dict=True
            )
            
            loss = outputs.loss
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„±ì„ ìœ„í•´)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            self.scheduler.step()
            
            # ì†ì‹¤ ê¸°ë¡
            total_loss += loss.item()
            num_batches += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'avg_loss': f"{total_loss/num_batches:.4f}",
                'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
            })
        
        return total_loss / num_batches
    
    def validate(self):
        """ê²€ì¦"""
        if self.val_loader is None:
            return None
        
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    decoder_input_ids=batch['decoder_input_ids'],
                    decoder_attention_mask=batch['decoder_attention_mask'],
                    labels=batch['labels'],
                    return_dict=True
                )
                
                total_loss += outputs.loss.item()
                num_batches += 1
        
        return total_loss / num_batches
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ë£¨í”„"""
        print(f"ğŸš€ Encoder-Decoder ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
        print(f"ğŸ“Š í•™ìŠµ ì„¤ì •:")
        print(f"   - ì—í¬í¬: {self.num_epochs}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.train_loader.batch_size}")
        print(f"   - í•™ìŠµë¥ : {self.optimizer.param_groups[0]['lr']}")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
        
        best_val_loss = float('inf')
        
        for epoch in range(self.num_epochs):
            print(f"\nğŸ“ˆ Epoch {epoch + 1}/{self.num_epochs}")
            
            # í•™ìŠµ
            train_loss = self.train_epoch()
            self.train_losses.append(train_loss)
            
            # ê²€ì¦
            val_loss = self.validate()
            if val_loss is not None:
                self.val_losses.append(val_loss)
                
                print(f"âœ… Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    self.save_checkpoint(f"best_model_epoch_{epoch+1}.pt")
                    print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (Val Loss: {val_loss:.4f})")
            else:
                print(f"âœ… Train Loss: {train_loss:.4f}")
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (epoch + 1) % 5 == 0:
                self.save_checkpoint(f"checkpoint_epoch_{epoch+1}.pt")
        
        print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        return self.train_losses, self.val_losses
    
    def save_checkpoint(self, filename):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs("encoder_decoder_checkpoints", exist_ok=True)
        filepath = os.path.join("encoder_decoder_checkpoints", filename)
        
        torch.save({
            'epoch': len(self.train_losses),
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.model.config,
        }, filepath)
    
    def generate_sample(self, source_text, max_length=50):
        """ìƒ˜í”Œ ë²ˆì—­ ìƒì„±"""
        self.model.eval()
        
        with torch.no_grad():
            # ì…ë ¥ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                source_text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=128
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ìƒì„±
            generated_ids = self.model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_length=max_length,
                do_sample=False  # Greedy decoding
            )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                generated_ids[0], 
                skip_special_tokens=True
            )
            
            return generated_text


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('mps' if torch.backends.mps.is_available() 
                         else 'cuda' if torch.cuda.is_available() 
                         else 'cpu')
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ (BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©)
    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
    print(f"ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {tokenizer.__class__.__name__}")
    
    # ëª¨ë¸ ì„¤ì •
    config = EncoderDecoderConfig(
        vocab_size=tokenizer.vocab_size,
        hidden_size=512,
        encoder_layers=4,
        decoder_layers=4,
        encoder_attention_heads=8,
        decoder_attention_heads=8,
        encoder_intermediate_size=2048,
        decoder_intermediate_size=2048,
        max_position_embeddings=512,
        pad_token_id=tokenizer.pad_token_id,
        bos_token_id=tokenizer.cls_token_id,
        eos_token_id=tokenizer.sep_token_id,
    )
    
    # ëª¨ë¸ ìƒì„±
    model = BERTEncoderTransformerDecoderModel(config)
    print(f"ğŸ¤– ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SimpleTranslationDataset(tokenizer, max_length=128)
    val_dataset = SimpleTranslationDataset(tokenizer, max_length=128)  # ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ë¶„ë¦¬í•´ì•¼ í•¨)
    
    print(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
    print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"   - ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = EncoderDecoderTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        batch_size=4,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì‹œì‘
        learning_rate=1e-4,
        num_epochs=5,
        device=device
    )
    
    # í•™ìŠµ ì „ ìƒ˜í”Œ ìƒì„±
    print(f"\nğŸ” í•™ìŠµ ì „ ìƒ˜í”Œ ìƒì„±:")
    sample_text = "Hello world"
    generated = trainer.generate_sample(sample_text)
    print(f"   ì…ë ¥: {sample_text}")
    print(f"   ì¶œë ¥: {generated}")
    
    # í•™ìŠµ ì‹¤í–‰
    train_losses, val_losses = trainer.train()
    
    # í•™ìŠµ í›„ ìƒ˜í”Œ ìƒì„±
    print(f"\nğŸ” í•™ìŠµ í›„ ìƒ˜í”Œ ìƒì„±:")
    generated = trainer.generate_sample(sample_text)
    print(f"   ì…ë ¥: {sample_text}")
    print(f"   ì¶œë ¥: {generated}")
    
    # ì—¬ëŸ¬ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    test_samples = [
        "How are you?",
        "Thank you very much",
        "Good morning",
        "I love programming"
    ]
    
    print(f"\nğŸ§ª ë‹¤ì–‘í•œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸:")
    for sample in test_samples:
        generated = trainer.generate_sample(sample)
        print(f"   {sample} â†’ {generated}")
    
    print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: encoder_decoder_checkpoints/")


if __name__ == "__main__":
    main()