"""
ê°„ë‹¨í•œ Encoder-Decoder í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

BERT Encoder + Transformer Decoder ëª¨ë¸ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸í•´ë´…ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from encoder_decoder_model import BERTEncoderTransformerDecoderModel, EncoderDecoderConfig

def test_encoder_decoder():
    """Encoder-Decoder ëª¨ë¸ ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('mps' if torch.backends.mps.is_available() 
                         else 'cuda' if torch.cuda.is_available() 
                         else 'cpu')
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
    print(f"ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    # ì‘ì€ ëª¨ë¸ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
    config = EncoderDecoderConfig(
        vocab_size=tokenizer.vocab_size,
        hidden_size=256,  # ì‘ê²Œ ì„¤ì •
        encoder_layers=2,
        decoder_layers=2,
        encoder_attention_heads=4,
        decoder_attention_heads=4,
        encoder_intermediate_size=512,
        decoder_intermediate_size=512,
        max_position_embeddings=128,
        pad_token_id=tokenizer.pad_token_id,
        bos_token_id=tokenizer.cls_token_id,
        eos_token_id=tokenizer.sep_token_id,
    )
    
    # ëª¨ë¸ ìƒì„±
    model = BERTEncoderTransformerDecoderModel(config).to(device)
    print(f"ğŸ¤– ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    english_text = "Hello world"
    korean_text = "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"
    
    # ì…ë ¥ í† í¬ë‚˜ì´ì§•
    encoder_inputs = tokenizer(
        english_text,
        max_length=32,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    decoder_inputs = tokenizer(
        f"[CLS] {korean_text}",
        max_length=32,
        padding='max_length', 
        truncation=True,
        return_tensors='pt'
    )
    
    labels = tokenizer(
        f"{korean_text} [SEP]",
        max_length=32,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    encoder_inputs = {k: v.to(device) for k, v in encoder_inputs.items()}
    decoder_inputs = {k: v.to(device) for k, v in decoder_inputs.items()}
    labels = {k: v.to(device) for k, v in labels.items()}
    
    print(f"\nğŸ” ì…ë ¥ ë°ì´í„°:")
    print(f"   Encoder ì…ë ¥: {english_text}")
    print(f"   Decoder ì…ë ¥: [CLS] {korean_text}")
    print(f"   ë ˆì´ë¸”: {korean_text} [SEP]")
    
    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§  ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸...")
    model.eval()
    
    with torch.no_grad():
        outputs = model(
            input_ids=encoder_inputs['input_ids'],
            attention_mask=encoder_inputs['attention_mask'],
            decoder_input_ids=decoder_inputs['input_ids'],
            decoder_attention_mask=decoder_inputs['attention_mask'],
            labels=labels['input_ids'],
            return_dict=True
        )
        
        print(f"âœ… ìˆœì „íŒŒ ì„±ê³µ!")
        print(f"   ì¶œë ¥ logits í¬ê¸°: {outputs.logits.shape}")
        print(f"   ì†ì‹¤: {outputs.loss.item():.4f}")
        
        # ì˜ˆì¸¡ í† í° í™•ì¸
        predicted_ids = outputs.logits.argmax(dim=-1)
        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
        print(f"   ì˜ˆì¸¡ í…ìŠ¤íŠ¸: {predicted_text}")
    
    # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸ (greedy decoding)
    print(f"\nğŸ¯ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    
    with torch.no_grad():
        # Encoder ì‹¤í–‰
        encoder_embeddings = model._get_embeddings(encoder_inputs['input_ids'])
        encoder_outputs = model.encoder(
            hidden_states=encoder_embeddings,
            attention_mask=encoder_inputs['attention_mask']
        )
        
        # Decoder ì‹œì‘ í† í°
        decoder_input_ids = torch.tensor(
            [[config.bos_token_id]], 
            device=device
        )
        
        # ìˆœì°¨ì ìœ¼ë¡œ í† í° ìƒì„± (ê°„ë‹¨í•œ greedy)
        generated_tokens = []
        max_length = 10
        
        for step in range(max_length):
            # Decoder ì„ë² ë”©
            decoder_embeddings = model._get_embeddings(decoder_input_ids)
            
            # Causal mask ìƒì„±
            seq_len = decoder_input_ids.size(1)
            causal_mask = model._create_causal_mask(seq_len, device)
            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)
            
            # Decoder ì‹¤í–‰
            decoder_outputs = model.decoder(
                hidden_states=decoder_embeddings,
                encoder_hidden_states=encoder_outputs.last_hidden_state,
                attention_mask=causal_mask,
                encoder_attention_mask=encoder_inputs['attention_mask']
            )
            
            # ë‹¤ìŒ í† í° ì˜ˆì¸¡
            logits = model.lm_head(decoder_outputs.last_hidden_state)
            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)
            
            # ìƒì„± ì¤‘ë‹¨ ì¡°ê±´
            if next_token.item() == config.eos_token_id:
                break
                
            generated_tokens.append(next_token.item())
            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=1)
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        if generated_tokens:
            generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            print(f"âœ… ìƒì„± ì„±ê³µ!")
            print(f"   ì…ë ¥: {english_text}")
            print(f"   ìƒì„±: {generated_text}")
        else:
            print(f"âš ï¸  í† í°ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
    
    print(f"\nğŸ‰ Encoder-Decoder ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"\nğŸ“‹ ëª¨ë¸ íŠ¹ì§•:")
    print(f"   â€¢ Encoder: BERT ìŠ¤íƒ€ì¼ (ì–‘ë°©í–¥ attention)")
    print(f"   â€¢ Decoder: Transformer ìŠ¤íƒ€ì¼ (causal + cross-attention)")
    print(f"   â€¢ ìš©ë„: ë²ˆì—­, ìš”ì•½, ì§ˆë¬¸ë‹µë³€")
    print(f"   â€¢ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

if __name__ == "__main__":
    test_encoder_decoder()