# Transformer Tutorial ì‹¤í–‰ ê°€ì´ë“œ

"Attention is All You Need" ë…¼ë¬¸ì˜ Transformerë¥¼ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ê³  ë‚´ë¶€ ë™ì‘ì„ ìƒì„¸íˆ ë¶„ì„í•˜ëŠ” íŠœí† ë¦¬ì–¼ì…ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
cd transformer_tutorial
pip install -r requirements.txt
```

### 2. ê¸°ë³¸ ì‹¤í–‰ (ì¶”ì²œ)
```bash
# ë‘ ë²„ì „ ëª¨ë‘ ì‹¤í–‰ (ê°€ì¥ êµìœ¡ì )
python main.py --version both --epochs 3 --debug --track_internals --visualize

# PyTorch ìˆœì • ë²„ì „ë§Œ (êµ¬í˜„ ì´í•´ìš©)
python main.py --version pytorch --epochs 5 --debug --visualize

# Hugging Face ë²„ì „ë§Œ (ì‹¤ë¬´ìš©)
python main.py --version huggingface --epochs 5 --track_internals --wandb
```

### 3. ë¶„ì„ ë„êµ¬ ë°ëª¨
```bash
# ì‹œê°í™” ë° ë¶„ì„ ë„êµ¬ í…ŒìŠ¤íŠ¸
python main.py --version analysis
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
transformer_tutorial/
â”œâ”€â”€ main.py                   # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt          # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ README.md                # í”„ë¡œì íŠ¸ ê°œìš”
â”œâ”€â”€ USAGE.md                 # ì´ íŒŒì¼
â”œâ”€â”€ pytorch_version/         # PyTorch ìˆœì • êµ¬í˜„
â”‚   â”œâ”€â”€ attention.py         # Multi-Head Attention (QKV ì¶”ì )
â”‚   â”œâ”€â”€ layers.py           # Encoder/Decoder ë ˆì´ì–´
â”‚   â”œâ”€â”€ model.py            # ì™„ì „í•œ Transformer ëª¨ë¸
â”‚   â””â”€â”€ train.py            # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ huggingface_version/    # Hugging Face êµ¬í˜„
â”‚   â”œâ”€â”€ model.py            # HF í˜¸í™˜ ëª¨ë¸ (ë‚´ë¶€ ì¶”ì )
â”‚   â”œâ”€â”€ custom_trainer.py   # ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ
â”‚   â””â”€â”€ train.py            # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ utils/                  # ê³µí†µ ìœ í‹¸ë¦¬í‹°
    â”œâ”€â”€ visualization.py    # ì‹œê°í™” ë„êµ¬
    â””â”€â”€ analysis.py         # ë¶„ì„ ë„êµ¬
```

## ğŸ¯ ê° ë²„ì „ë³„ íŠ¹ì§•

### A) PyTorch ìˆœì • ë²„ì „ (Educational)
**ëª©ì **: Transformer ë‚´ë¶€ ë™ì‘ ì™„ì „ ì´í•´

**ì£¼ìš” ê¸°ëŠ¥**:
- âœ¨ **QKV ê³„ì‚° ê³¼ì • ìƒì„¸ ì¶”ì **: Query, Key, Value ë³€í™˜ ë‹¨ê³„ë³„ ì¶œë ¥
- ğŸ” **Attention ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„**: Scaled Dot-Product Attention ì¤‘ê°„ê°’ ëª¨ë‹ˆí„°ë§
- ğŸ“Š **Multi-Head ì‹œê°í™”**: ê° í—¤ë“œë³„ attention pattern íˆíŠ¸ë§µ
- âš¡ **Linear/Non-linear ë³€í™˜ ì¶”ì **: ReLU, Layer Norm ì ìš© ì „í›„ ë¹„êµ
- ğŸ“ˆ **ë°ì´í„° íë¦„ ì™„ì „ ì¶”ì **: Encoder â†’ Decoder â†’ Output ì „ì²´ íŒŒì´í”„ë¼ì¸

**ì‹¤í–‰ ì˜ˆì‹œ**:
```bash
# ë””ë²„ê·¸ ëª¨ë“œë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸
python main.py --version pytorch --epochs 5 --debug --visualize \
    --batch_size 8 --hidden_size 256

# ì‘ì€ ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python main.py --version pytorch --epochs 2 --debug \
    --hidden_size 128 --num_layers 2
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
=== Multi-Head Attention Forward Pass ===
Input shapes - Q: torch.Size([2, 10, 256]), K: torch.Size([2, 10, 256])
1. Linear Transformations:
Q after W_q: torch.Size([2, 10, 256]), mean: 0.0234, std: 0.8901
2. Reshape for Multi-Head:
Q reshaped: torch.Size([2, 8, 10, 32])
3. Attention Output: torch.Size([2, 8, 10, 32])
```

### B) Hugging Face ë²„ì „ (Production)
**ëª©ì **: ì‹¤ë¬´í˜• êµ¬í˜„ + ê³ ê¸‰ ë¶„ì„

**ì£¼ìš” ê¸°ëŠ¥**:
- ğŸ­ **Production-Ready**: í† í¬ë‚˜ì´ì €, ë°ì´í„°ë¡œë”, íŠ¸ë ˆì´ë„ˆ í’€ìŠ¤íƒ
- ğŸ£ **Hook ê¸°ë°˜ ì¶”ì **: ì‹¤ì‹œê°„ ë‚´ë¶€ í…ì„œ ê°’ ëª¨ë‹ˆí„°ë§
- ğŸ“Š **WandB í†µí•©**: í•™ìŠµ ê³¼ì • ì‹¤ì‹œê°„ ì‹œê°í™”
- âš¡ **Mixed Precision**: GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ
- ğŸ”„ **ë¶„ì‚° í•™ìŠµ ì§€ì›**: Multi-GPU í™˜ê²½ ëŒ€ì‘

**ì‹¤í–‰ ì˜ˆì‹œ**:
```bash
# WandB ë¡œê¹…ê³¼ í•¨ê»˜ ì‹¤í–‰
python main.py --version huggingface --epochs 10 --track_internals \
    --wandb --wandb_project my-transformer-experiment

# í° ëª¨ë¸ ì‹¤í—˜
python main.py --version huggingface --epochs 5 --track_internals \
    --hidden_size 512 --num_layers 6 --batch_size 32
```

## ğŸ“Š ë¶„ì„ ë° ì‹œê°í™”

### 1. Attention Pattern ë¶„ì„
```bash
# ì‹¤í–‰ í›„ ìƒì„±ë˜ëŠ” ì‹œê°í™”
results_pytorch/visualizations/attention/
â”œâ”€â”€ attention_step_100.png    # íŠ¹ì • ìŠ¤í…ì˜ attention íŒ¨í„´
â”œâ”€â”€ attention_evolution.png   # í•™ìŠµ ê³¼ì •ì—ì„œ ë³€í™”
â””â”€â”€ multihead_comparison.png  # í—¤ë“œê°„ ë¹„êµ
```

### 2. QKV ê°’ ë³€í™” ì¶”ì 
```bash
results_huggingface/visualizations/qkv/
â”œâ”€â”€ qkv_evolution.png         # ë ˆì´ì–´ë³„ QKV í‰ê· ê°’ ë³€í™”
â”œâ”€â”€ layer_contributions.png   # ê° ë ˆì´ì–´ì˜ ê¸°ì—¬ë„
â””â”€â”€ gradient_flow.png         # Gradient íë¦„ ë¶„ì„
```

### 3. í•™ìŠµ ê³¡ì„ 
```bash
results_*/
â”œâ”€â”€ training_curves.png       # Loss, ë©”íŠ¸ë¦­ ê³¡ì„ 
â”œâ”€â”€ attention_weights_*.png   # Attention weight íˆíŠ¸ë§µ
â””â”€â”€ analysis_report.json     # ì¢…í•© ë¶„ì„ ê²°ê³¼
```

## ğŸ”¬ ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥

### 1. ì¢…í•© ë¶„ì„ ì‹¤í–‰
```python
from utils.analysis import TransformerAnalyzer
from utils.visualization import TransformerVisualizer

# ëª¨ë¸ ë¡œë“œ í›„
analyzer = TransformerAnalyzer(model)
results = analyzer.comprehensive_analysis(
    input_ids, target_ids, attention_mask, tokens,
    save_path="detailed_analysis.json"
)

# ì‹œê°í™”
visualizer = TransformerVisualizer("my_viz")
visualizer.plot_attention_heatmap(attention_weights, tokens)
```

### 2. ì»¤ìŠ¤í…€ ë¶„ì„
```python
# QKV í†µê³„ ë¶„ì„
qkv_stats = analyzer.analyze_qkv_statistics(input_ids)

# Attention pattern ë¶„ë¥˜
attention_analysis = analyzer.analyze_attention_patterns(input_ids, tokens=tokens)

# Gradient flow ë¶„ì„
gradient_analysis = analyzer.analyze_gradient_flow(input_ids, target_ids)
```

## ğŸ“ í•™ìŠµ ë¡œë“œë§µ

### ì´ˆê¸‰: Transformer ê¸°ë³¸ ì´í•´
1. **PyTorch ë²„ì „ ì‹¤í–‰**: `--version pytorch --debug`
2. **QKV ê³¼ì • ê´€ì°°**: ì½˜ì†” ì¶œë ¥ì—ì„œ ê° ë‹¨ê³„ë³„ ê°’ ë³€í™” í™•ì¸
3. **Attention ì‹œê°í™”**: ìƒì„±ëœ íˆíŠ¸ë§µìœ¼ë¡œ í† í°ê°„ ê´€ê³„ ì´í•´

### ì¤‘ê¸‰: ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„
1. **ë‘ ë²„ì „ ë¹„êµ**: `--version both`ë¡œ êµ¬í˜„ ì°¨ì´ì  ì´í•´
2. **í•™ìŠµ ê³¼ì • ì¶”ì **: QKV evolution ê·¸ë˜í”„ë¡œ í•™ìŠµ ì—­í•™ ë¶„ì„
3. **Attention pattern ë¶„ë¥˜**: Local vs Global vs Focused íŒ¨í„´ êµ¬ë¶„

### ê³ ê¸‰: ì‹¤ë¬´ ì‘ìš©
1. **ëŒ€ìš©ëŸ‰ ì‹¤í—˜**: ë” í° ëª¨ë¸, ë°ì´í„°ì…‹ìœ¼ë¡œ í™•ì¥
2. **ì„±ëŠ¥ ìµœì í™”**: Mixed Precision, Gradient Accumulation
3. **ì»¤ìŠ¤í…€ ë¶„ì„**: íŠ¹ì • íƒœìŠ¤í¬ì— ë§ëŠ” ë¶„ì„ ë„êµ¬ ê°œë°œ

## ğŸ› ï¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. ëª¨ë¸ í¬ê¸° ì¡°ì •
```bash
# ì†Œí˜• ëª¨ë¸ (ë¹ ë¥¸ ì‹¤í—˜)
python main.py --hidden_size 128 --num_layers 2 --num_heads 4

# ëŒ€í˜• ëª¨ë¸ (ì„±ëŠ¥ ì‹¤í—˜)
python main.py --hidden_size 512 --num_layers 8 --num_heads 16
```

### 2. ë°ì´í„°ì…‹ í¬ê¸° ì¡°ì •
```bash
# ì‘ì€ ë°ì´í„°ì…‹ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
python main.py --train_size 1000 --eval_size 200

# í° ë°ì´í„°ì…‹ (ë³¸ê²© ì‹¤í—˜)
python main.py --train_size 10000 --eval_size 2000
```

### 3. ë¶„ì„ ì„¤ì •
```bash
# ìµœëŒ€ ë””ë²„ê¹…
python main.py --version pytorch --debug --visualize \
    --epochs 3 --batch_size 4

# ìµœëŒ€ ì¶”ì 
python main.py --version huggingface --track_internals \
    --wandb --epochs 10
```

## ğŸ”§ ë¬¸ì œí•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python main.py --batch_size 4 --hidden_size 128

# Gradient accumulation ì‚¬ìš©
python main.py --version huggingface --batch_size 8 --gradient_accumulation_steps 4
```

### ëŠë¦° ì‹¤í–‰
```bash
# ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘
python main.py --epochs 2 --train_size 500 --eval_size 100

# CPUì—ì„œ ì‹¤í–‰ì‹œ workers 0ìœ¼ë¡œ
python main.py --dataloader_num_workers 0
```

### ì‹œê°í™” ë¬¸ì œ
```bash
# matplotlib backend ì„¤ì •
export MPLBACKEND=Agg  # headless í™˜ê²½
python main.py --visualize
```

## ğŸ“ˆ í™•ì¥ ì•„ì´ë””ì–´

### 1. ì‹¤ì œ ë°ì´í„°ì…‹ ì ìš©
- WMT ë²ˆì—­ ë°ì´í„°ì…‹
- WikiText ì–¸ì–´ëª¨ë¸ë§
- GLUE íƒœìŠ¤í¬ fine-tuning

### 2. ê³ ê¸‰ ê¸°ë²• ì¶”ê°€
- Gradient checkpointing
- Dynamic attention
- Sparse attention patterns

### 3. ë¶„ì„ ë„êµ¬ í™•ì¥
- Attention entropy ë¶„ì„
- Layer-wise learning rate
- Probing ì‹¤í—˜

## ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸

ì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ ë‹¤ìŒì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **QKV ë³€í™˜**: Query, Key, Valueê°€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ê³  ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€
2. **Multi-Headì˜ íš¨ê³¼**: ê° í—¤ë“œê°€ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹
3. **Attention ì§„í™”**: í•™ìŠµ ê³¼ì •ì—ì„œ attention íŒ¨í„´ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€
4. **Layer ê¸°ì—¬ë„**: ê° ë ˆì´ì–´ê°€ ìµœì¢… ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
5. **êµ¬í˜„ ì°¨ì´ì **: From-scratch vs Library êµ¬í˜„ì˜ ì¥ë‹¨ì 

**í•µì‹¬**: TransformerëŠ” ë‹¨ìˆœí•œ attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì¡°í•©ì´ì§€ë§Œ, ê·¸ ìƒí˜¸ì‘ìš©ì´ ë§Œë“¤ì–´ë‚´ëŠ” ë³µì¡ì„±ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.