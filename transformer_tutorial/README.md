# Transformer Tutorial: "Attention is All You Need" êµ¬í˜„ê³¼ ë¶„ì„

ì´ í”„ë¡œì íŠ¸ëŠ” "Attention is All You Need" ë…¼ë¬¸ì˜ Transformerë¥¼ **ë‘ ê°€ì§€ ë°©ì‹**ìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ ë‚´ë¶€ ë™ì‘ì„ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.

## âš¡ ê°€ì¥ ë¹ ë¥¸ ì‹œì‘ ë°©ë²•

### ğŸ¤– ì™„ì „ ìë™ ì‹¤í–‰ (ì¶”ì²œ!)
```bash
# í•œ ë²ˆì˜ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜ë¶€í„° ì‹¤í–‰ê¹Œì§€!
cd /Users/gimjunseog/projects/linear-algebra/transformer_tutorial
./setup_and_run.sh
```

**Windows ì‚¬ìš©ì:**
```cmd
cd /Users/gimjunseog/projects/linear-algebra/transformer_tutorial
setup_and_run.bat
```

### ğŸ”§ ìˆ˜ë™ ì„¤ì • (ë‹¨ê³„ë³„)
```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /Users/gimjunseog/projects/linear-algebra/transformer_tutorial

# 2. ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” (í•œ ë²ˆë§Œ)
python3 -m venv transformer_env
source transformer_env/bin/activate  # Windows: transformer_env\Scripts\activate

# 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•œ ë²ˆë§Œ)
pip install -r requirements.txt

# 4. ì‹¤í–‰!
python main.py --version both --epochs 2 --train_size 500
```

## âš¡ ë¹ ë¥¸ ì‹œì‘ (Quick Start)

### 1. í”„ë¡œì íŠ¸ í´ë¡  ë° ì´ë™
```bash
cd /Users/gimjunseog/projects/linear-algebra/transformer_tutorial
```

### 2. Python ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv transformer_env

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source transformer_env/bin/activate  # macOS/Linux
# ë˜ëŠ” Windowsì˜ ê²½ìš°: transformer_env\Scripts\activate

# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸ (í”„ë¡¬í”„íŠ¸ ì•ì— (transformer_env) í‘œì‹œë¨)
which python  # ê°€ìƒí™˜ê²½ì˜ python ê²½ë¡œê°€ í‘œì‹œë˜ì–´ì•¼ í•¨
```

### 3. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# requirements.txtì˜ íŒ¨í‚¤ì§€ë“¤ ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt

# ì„¤ì¹˜ í™•ì¸
pip list | grep torch
pip list | grep transformers
```

### 4. ì¦‰ì‹œ ì‹¤í–‰ (ê¶Œì¥)
```bash
# ğŸš€ ê¸°ë³¸ ì‹¤í–‰ - ë‘ ë²„ì „ ëª¨ë‘ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸
python main.py --version both --epochs 2 --train_size 500 --eval_size 100

# ğŸ” PyTorch ë²„ì „ ìƒì„¸ ë¶„ì„
python main.py --version pytorch --epochs 3 --debug --visualize

# ğŸ­ Hugging Face ë²„ì „ ì‹¤ë¬´ ì‹¤í–‰
python main.py --version huggingface --epochs 3 --track_internals

# ğŸ“Š ë¶„ì„ ë„êµ¬ ë°ëª¨
python main.py --version analysis
```

### 5. ê°€ìƒí™˜ê²½ ì¢…ë£Œ (ì‘ì—… ì™„ë£Œ í›„)
```bash
deactivate
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
transformer_tutorial/
â”œâ”€â”€ pytorch_version/          # PyTorch ìˆœì • êµ¬í˜„
â”‚   â”œâ”€â”€ model.py             # Transformer ëª¨ë¸ (from scratch)
â”‚   â”œâ”€â”€ attention.py         # Multi-Head Attention êµ¬í˜„
â”‚   â”œâ”€â”€ layers.py            # Encoder/Decoder ë ˆì´ì–´
â”‚   â””â”€â”€ train.py             # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ huggingface_version/      # Hugging Face êµ¬í˜„
â”‚   â”œâ”€â”€ model.py             # HF ê¸°ë°˜ ëª¨ë¸
â”‚   â”œâ”€â”€ custom_trainer.py    # ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ
â”‚   â””â”€â”€ train.py             # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ utils/                    # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ data.py              # ë°ì´í„° ë¡œë”
â”‚   â”œâ”€â”€ tokenizer.py         # í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ visualization.py     # ì‹œê°í™” ë„êµ¬
â”‚   â””â”€â”€ analysis.py          # QKV ë¶„ì„ ë„êµ¬
â””â”€â”€ requirements.txt
```

## ğŸ“‹ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- **Python 3.8+** (ê¶Œì¥: Python 3.9+)
- **pip** (Python íŒ¨í‚¤ì§€ ê´€ë¦¬ì)
- **8GB RAM ì´ìƒ** (ëª¨ë¸ í•™ìŠµìš©)
- **CUDA ì§€ì› GPU** (ì„ íƒì‚¬í•­, ì†ë„ í–¥ìƒ)

### Python ë²„ì „ í™•ì¸
```bash
python3 --version  # Python 3.8+ í™•ì¸
pip --version      # pip í™•ì¸
```

### ğŸš€ ê°€ìƒí™˜ê²½ ì„¤ì¹˜ ë° ì‹¤í–‰ (í•„ìˆ˜!)

**ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :**
- ì‹œìŠ¤í…œ Python í™˜ê²½ ë³´í˜¸
- íŒ¨í‚¤ì§€ ì¶©ëŒ ë°©ì§€
- í”„ë¡œì íŠ¸ë³„ ë…ë¦½ì  í™˜ê²½ ê´€ë¦¬

**1ë‹¨ê³„: ê°€ìƒí™˜ê²½ ìƒì„±**
```bash
# í˜„ì¬ ë””ë ‰í† ë¦¬ì— ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv transformer_env

# ë˜ëŠ” ì‹œìŠ¤í…œ ì „ì—­ì— ìƒì„±í•˜ê³  ì‹¶ë‹¤ë©´
python3 -m venv ~/venvs/transformer_env
```

**2ë‹¨ê³„: ê°€ìƒí™˜ê²½ í™œì„±í™”**
```bash
# macOS/Linux
source transformer_env/bin/activate

# Windows
transformer_env\Scripts\activate

# í™œì„±í™” í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— (transformer_env) í‘œì‹œë¨)
which python  # ê°€ìƒí™˜ê²½ Python ê²½ë¡œ í™•ì¸
```

**3ë‹¨ê³„: íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)
pip install --upgrade pip
pip install -r requirements.txt

# ì„¤ì¹˜ í™•ì¸
pip list | grep torch
```

**4ë‹¨ê³„: í”„ë¡œê·¸ë¨ ì‹¤í–‰**
```bash
# ê¸°ë³¸ ì‹¤í–‰
python main.py

# ìƒì„¸ ì˜µì…˜ìœ¼ë¡œ ì‹¤í–‰
python main.py --version both --epochs 3 --train_size 1000
```

**5ë‹¨ê³„: ê°€ìƒí™˜ê²½ ì¢…ë£Œ (ì‘ì—… ì™„ë£Œ í›„)**
```bash
deactivate
```

### ğŸ”„ ì¬ì‹¤í–‰í•  ë•Œ (ë‹¤ìŒ ë²ˆë¶€í„°)
```bash
# 1. ë””ë ‰í† ë¦¬ ì´ë™
cd /Users/gimjunseog/projects/linear-algebra/transformer_tutorial

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™” (ë§¤ë²ˆ í•„ìš”)
source transformer_env/bin/activate

# 3. ì‹¤í–‰
python main.py --version both

# 4. ì¢…ë£Œ
deactivate
```

## ğŸ“¦ ì£¼ìš” ì˜ì¡´ì„± íŒ¨í‚¤ì§€

í•µì‹¬ íŒ¨í‚¤ì§€ë“¤:
- **PyTorch 2.0+**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **Transformers 4.30+**: Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸
- **matplotlib, seaborn**: ì‹œê°í™”
- **wandb**: ì‹¤í—˜ ì¶”ì  (ì„ íƒì‚¬í•­)
- **datasets**: ë°ì´í„°ì…‹ ë¡œë”©

## ğŸ¯ ì‹¤í–‰ ì˜µì…˜

### A) PyTorch ìˆœì • ë²„ì „ (From-Scratch)
- **QKV ê³„ì‚° ê³¼ì •**: Query, Key, Value ë³€í™˜ ì¶”ì 
- **Attention ë©”ì»¤ë‹ˆì¦˜**: Scaled Dot-Product Attention ë‹¨ê³„ë³„ ë¶„ì„
- **Multi-Head**: ê° í—¤ë“œë³„ attention weight ì‹œê°í™”
- **Linear/Non-linear ë³€í™˜**: ì–¸ì œ, ì–´ë””ì„œ ì¼ì–´ë‚˜ëŠ”ì§€ ì¶”ì 
- **ë°ì´í„° íë¦„**: Encoder â†’ Decoder â†’ Output ì „ì²´ íŒŒì´í”„ë¼ì¸

### B) Hugging Face ë²„ì „ (Production-Ready)
- **Hook ê¸°ë°˜ ë¶„ì„**: ë‚´ë¶€ í…ì„œ ê°’ ì‹¤ì‹œê°„ ì¶”ì 
- **ì‹¤ë¬´í˜• êµ¬í˜„**: í† í¬ë‚˜ì´ì €, ë°ì´í„°ì…‹, íŠ¸ë ˆì´ë„ˆ í’€ìŠ¤íƒ
- **ì„±ëŠ¥ ìµœì í™”**: Mixed Precision, Gradient Accumulation
- **ë¶„ì‚° í•™ìŠµ**: Multi-GPU ì§€ì›

## ğŸ” ì£¼ìš” ë¶„ì„ ê¸°ëŠ¥

1. **QKV ë³€í™” ì¶”ì **: ê° ë ˆì´ì–´ì—ì„œ Query, Key, Value ê°’ ë³€í™”
2. **Attention Weight ì‹œê°í™”**: ë‹¨ì–´ ê°„ attention íŒ¨í„´ íˆíŠ¸ë§µ
3. **FFN ì¤‘ê°„ê°’**: Feed-Forward Network ë‚´ë¶€ í™œì„±í™” ë¶„ì„
4. **Gradient Flow**: ì—­ì „íŒŒ ê³¼ì •ì—ì„œ gradient ë³€í™”
5. **í† í°ë³„ ì„ë² ë”©**: ì…ë ¥ í† í°ì´ ì–´ë–»ê²Œ ë³€í™˜ë˜ëŠ”ì§€ ì¶”ì 

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

#### Option A: ê°„ë‹¨í•œ ë°©ë²• (ê¶Œì¥)
```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /Users/gimjunseog/projects/linear-algebra/transformer_tutorial

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python3 -m venv transformer_env
source transformer_env/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì¦‰ì‹œ ì‹¤í–‰
pip install -r requirements.txt
python main.py --version both --epochs 2 --train_size 500
```

#### Option B: ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ì •
```bash
# 1ï¸âƒ£ Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv transformer_env

# 2ï¸âƒ£ ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
source transformer_env/bin/activate
echo "ê°€ìƒí™˜ê²½ í™œì„±í™”ë¨: $(which python)"

# 3ï¸âƒ£ pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip

# 4ï¸âƒ£ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch>=2.0.0 torchvision>=0.15.0
pip install transformers>=4.30.0 tokenizers>=0.13.0
pip install matplotlib seaborn numpy pandas tqdm

# 5ï¸âƒ£ ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 6ï¸âƒ£ ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(f'PyTorch ë²„ì „: {torch.__version__}')"
python -c "import transformers; print(f'Transformers ë²„ì „: {transformers.__version__}')"
```

### 2. ì‹¤í–‰ ì˜µì…˜

#### ğŸ¯ ëª©ì ë³„ ì‹¤í–‰ ê°€ì´ë“œ

**A) Transformer ê¸°ë³¸ ì´í•´ (ì´ˆë³´ììš©)**
```bash
# PyTorch ìˆœì • êµ¬í˜„ìœ¼ë¡œ ë‚´ë¶€ ë™ì‘ ì´í•´
python main.py --version pytorch --epochs 3 --debug --visualize \
    --batch_size 8 --train_size 1000 --eval_size 200
```

**B) ì‹¤ë¬´ êµ¬í˜„ í•™ìŠµ (ì¤‘ê¸‰ììš©)**
```bash
# Hugging Face ë²„ì „ìœ¼ë¡œ production-ready ì½”ë“œ í•™ìŠµ
python main.py --version huggingface --epochs 5 --track_internals \
    --batch_size 16 --train_size 2000 --eval_size 500
```

**C) ì™„ì „í•œ ë¹„êµ ë¶„ì„ (ê³ ê¸‰ììš©)**
```bash
# ë‘ ë²„ì „ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ì°¨ì´ì  ë¶„ì„
python main.py --version both --epochs 5 --debug --track_internals --visualize \
    --batch_size 12 --train_size 3000 --eval_size 600
```

**D) ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**
```bash
# ë¶„ì„ ë„êµ¬ ë°ëª¨ (ì‹¤ì œ í•™ìŠµ ì—†ì´ ì‹œê°í™” í™•ì¸)
python main.py --version analysis

# ëª¨ë¸ êµ¬ì¡° í…ŒìŠ¤íŠ¸
python main.py --version test
```

### 3. WandB ë¡œê¹… (ì„ íƒì‚¬í•­)
```bash
# WandB ê³„ì • ì„¤ì • (ì²˜ìŒ í•œ ë²ˆë§Œ)
pip install wandb
wandb login  # ì›¹ë¸Œë¼ìš°ì €ì—ì„œ API í‚¤ ì…ë ¥

# WandBì™€ í•¨ê»˜ ì‹¤í–‰
python main.py --version huggingface --epochs 10 --track_internals \
    --wandb --wandb_project my-transformer-study
```

## ï¿½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ê°€ìƒí™˜ê²½ ê´€ë ¨ ë¬¸ì œ

**Q: ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•„ìš”**
```bash
# í•´ê²°ë°©ë²• 1: ì „ì²´ ê²½ë¡œë¡œ í™œì„±í™”
source /Users/gimjunseog/projects/linear-algebra/transformer_tutorial/transformer_env/bin/activate

# í•´ê²°ë°©ë²• 2: ê°€ìƒí™˜ê²½ ì¬ìƒì„±
rm -rf transformer_env
python3 -m venv transformer_env
source transformer_env/bin/activate
```

**Q: pip installì´ ì‹¤íŒ¨í•´ìš”**
```bash
# pip ì—…ê·¸ë ˆì´ë“œ í›„ ì¬ì‹œë„
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt

# ë˜ëŠ” ê°œë³„ ì„¤ì¹˜
pip install torch torchvision
pip install transformers
pip install matplotlib seaborn numpy pandas
```

**Q: (transformer_env) í‘œì‹œê°€ ì•ˆ ë³´ì—¬ìš”**
```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
echo $VIRTUAL_ENV  # ê²½ë¡œê°€ í‘œì‹œë˜ì–´ì•¼ í•¨
which python       # transformer_env ë‚´ì˜ python ê²½ë¡œê°€ í‘œì‹œë˜ì–´ì•¼ í•¨
```

### ì‹¤í–‰ ê´€ë ¨ ë¬¸ì œ

**Q: CUDA/GPU ê´€ë ¨ ì˜¤ë¥˜**
```bash
# CPU ëª¨ë“œë¡œ ì‹¤í–‰
python main.py --version pytorch --epochs 2 --batch_size 4

# GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

**Q: ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# ë°°ì¹˜ í¬ê¸°ì™€ ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
python main.py --version pytorch --epochs 2 --batch_size 4 \
    --hidden_size 128 --train_size 500 --eval_size 100
```

**Q: matplotlib ë°±ì—”ë“œ ì˜¤ë¥˜**
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ ì‹¤í–‰
export MPLBACKEND=Agg
python main.py --version pytorch --visualize
```

### íŒ¨í‚¤ì§€ ê´€ë ¨ ë¬¸ì œ

**Q: transformers ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ**
```bash
# íŠ¹ì • ë²„ì „ ì„¤ì¹˜
pip install transformers==4.30.0 tokenizers==0.13.0

# ë˜ëŠ” ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade transformers tokenizers
```

**Q: numpy/pandas ì˜¤ë¥˜**
```bash
# í˜¸í™˜ ê°€ëŠ¥í•œ ë²„ì „ ì„¤ì¹˜
pip install numpy==1.24.0 pandas==2.0.0
```

## ğŸ§ª ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

### ì„¤ì¹˜ í™•ì¸
```bash
# ê°€ìƒí™˜ê²½ì—ì„œ ì‹¤í–‰
source transformer_env/bin/activate

# ê¸°ë³¸ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
python -c "
import torch
import transformers
import matplotlib.pyplot as plt
import numpy as np
print('âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì •ìƒ ì„í¬íŠ¸ë¨')
print(f'PyTorch: {torch.__version__}')
print(f'Transformers: {transformers.__version__}')
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
"
```

### ë¹ ë¥¸ ë™ì‘ í…ŒìŠ¤íŠ¸
```bash
# 1ë¶„ ë‚´ ì™„ë£Œë˜ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
python main.py --version test

# 5ë¶„ ë‚´ ì™„ë£Œë˜ëŠ” ê¸°ë³¸ ì‹¤í–‰
python main.py --version pytorch --epochs 1 --train_size 200 --eval_size 50 --debug
```

## ğŸ“‹ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:
- [ ] Python 3.8+ ì„¤ì¹˜ë¨
- [ ] ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”ë¨ `(transformer_env)`
- [ ] requirements.txt íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¨
- [ ] í˜„ì¬ ë””ë ‰í† ë¦¬ê°€ `transformer_tutorial/`ì„
- [ ] ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ 1GB ì´ìƒ

ê¶Œì¥ ì²« ì‹¤í–‰:
```bash
# âœ… ì´ ëª…ë ¹ì–´ë¡œ ì‹œì‘í•˜ì„¸ìš”!
python main.py --version both --epochs 2 --train_size 500 --eval_size 100 --debug
```

## ğŸŒŸ ì‹¤í–‰ í›„ í™•ì¸í•  íŒŒì¼ë“¤

ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©´ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
results_pytorch/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pt              # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”‚   â””â”€â”€ checkpoint_epoch_*.pt      # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ attention_plots/
â”‚   â”‚   â””â”€â”€ attention_evolution.png  # Attention íŒ¨í„´ ë³€í™”
â”‚   â”œâ”€â”€ training_curves.png         # í•™ìŠµ ê³¡ì„ 
â”‚   â””â”€â”€ qkv_evolution.png          # QKV ê°’ ë³€í™”
â””â”€â”€ logs/                          # í•™ìŠµ ë¡œê·¸

results_huggingface/
â”œâ”€â”€ pytorch_model.bin              # ì €ì¥ëœ ëª¨ë¸
â”œâ”€â”€ config.json                    # ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ training_args.bin              # í•™ìŠµ ì„¤ì •
â””â”€â”€ visualizations/
    â”œâ”€â”€ attention/
    â”‚   â””â”€â”€ attention_step_*.png    # ìŠ¤í…ë³„ attention
    â””â”€â”€ qkv/
        â””â”€â”€ qkv_evolution.png      # QKV ë³€í™” ì¶”ì 
```

## ï¿½ ë¬¸ì œ í•´ê²° (Troubleshooting)

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

**1. Python ë²„ì „ ë¬¸ì œ**
```bash
# Python 3.8+ í™•ì¸
python3 --version

# ì‹œìŠ¤í…œì— ì—¬ëŸ¬ Pythonì´ ì„¤ì¹˜ëœ ê²½ìš°
which python3
which pip3
```

**2. ê°€ìƒí™˜ê²½ í™œì„±í™” ì•ˆë¨**
```bash
# ê°€ìƒí™˜ê²½ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
ls transformer_env/bin/  # activate íŒŒì¼ í™•ì¸

# ê¶Œí•œ ë¬¸ì œì¸ ê²½ìš°
chmod +x transformer_env/bin/activate
source transformer_env/bin/activate
```

**3. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì˜¤ë¥˜**
```bash
# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip

# ìºì‹œ í´ë¦¬ì–´
pip cache purge

# ê°œë³„ ì„¤ì¹˜ ì‹œë„
pip install torch torchvision torchaudio
pip install transformers
```

**4. CUDA/GPU ë¬¸ì œ**
```bash
# CUDA ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"

# CPUë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš°
python main.py --device cpu
```

**5. ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ ì‹¤í–‰
python main.py --batch_size 8 --train_size 100
```

**6. ì™„ì „ ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°**
```bash
# ê°€ìƒí™˜ê²½ ì‚­ì œ í›„ ì¬ìƒì„±
rm -rf transformer_env
python3 -m venv transformer_env
source transformer_env/bin/activate
pip install -r requirements.txt
```

### ë” ìì„¸í•œ ì„¤ì¹˜ ê°€ì´ë“œ
ğŸ“– ìƒì„¸í•œ ì„¤ì¹˜ ë°©ë²•ì€ [INSTALL.md](INSTALL.md) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”!

## ï¿½ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

1. **ê²°ê³¼ ë¶„ì„**: ìƒì„±ëœ ì‹œê°í™” íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì—¬ Transformer ë‚´ë¶€ ë™ì‘ ì´í•´
2. **ì½”ë“œ ì½ê¸°**: `pytorch_version/`ê³¼ `huggingface_version/` ì½”ë“œ ë¹„êµ ë¶„ì„
3. **ì‹¤í—˜ í™•ì¥**: ë” í° ëª¨ë¸, ë” ë§ì€ ë°ì´í„°ë¡œ ì‹¤í—˜
4. **ì‹¤ì œ ì ìš©**: ë³¸ì¸ì˜ í”„ë¡œì íŠ¸ì— í•™ìŠµí•œ ë‚´ìš© ì ìš©